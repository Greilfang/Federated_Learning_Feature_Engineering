{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-810c308a8d30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def save_model(path, model, optimizer):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict()\n",
    "    }, path)\n",
    "\n",
    "\n",
    "def load_model(path, model, device, mode, optimizer=None, lr=None):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    if optimizer is not None:\n",
    "        optimizer_state = checkpoint['optimizer_state']\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "        if lr is not None:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        print(optimizer)\n",
    "    \n",
    "    if mode == \"train\":\n",
    "        model.train()\n",
    "    elif mode == \"eval\":\n",
    "        model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "refine_flag =True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(MLP, self).__init__()\n",
    "            n_cores =512\n",
    "            \n",
    "            self.linear =torch. nn.Sequential(\n",
    "                torch.nn.Linear(input_size, n_cores),\n",
    "                torch.nn.Dropout(0.5),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                torch.nn.Linear(n_cores, 2),\n",
    "            )\n",
    " \n",
    "        def forward(self, x):\n",
    "            out = self.linear(x)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum (22770, 800)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.21710479259490967 accum:38.6446533203125\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.2136761099100113 accum:38.03434753417969\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.21763454377651215 accum:38.738948822021484\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.21137942373752594 accum:37.62553787231445\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.21372833847999573 accum:38.043643951416016\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2156635969877243 accum:38.388118743896484\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.218818798661232 accum:38.949745178222656\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.21638815104961395 accum:38.51708984375\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.21165989339351654 accum:37.67546081542969\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.21229566633701324 accum:37.78862762451172\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.21477653086185455 accum:38.230220794677734\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.20935919880867004 accum:37.26593780517578\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.21304550766944885 accum:37.92210006713867\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.21828261017799377 accum:38.854305267333984\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.21276801824569702 accum:37.87270736694336\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.2113923877477646 accum:37.627845764160156\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.2211698442697525 accum:39.36823272705078\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.22423455119132996 accum:39.91374969482422\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.2177770435810089 accum:38.764312744140625\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.21465879678726196 accum:38.209266662597656\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2176010012626648 accum:38.73297882080078\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.2136315107345581 accum:38.02640914916992\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.21811048686504364 accum:38.823665618896484\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.2261440008878708 accum:40.253631591796875\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.2118247002363205 accum:37.704795837402344\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.2212173044681549 accum:39.376678466796875\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.21745748817920685 accum:38.70743179321289\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.21397580206394196 accum:38.08769226074219\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2170933336019516 accum:38.64261245727539\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.21439331769943237 accum:38.162010192871094\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.2174016386270523 accum:38.69749069213867\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.20964960753917694 accum:37.317630767822266\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.21606820821762085 accum:38.460140228271484\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.21292725205421448 accum:37.90105056762695\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.21069492399692535 accum:37.50369644165039\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.21571864187717438 accum:38.397918701171875\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.20958302915096283 accum:37.30577850341797\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.21398186683654785 accum:38.08877182006836\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.21288348734378815 accum:37.89326095581055\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.21242815256118774 accum:37.81221008300781\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.2133287638425827 accum:37.97251892089844\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.20759743452072144 accum:36.95234298706055\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.21269820630550385 accum:37.86027908325195\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.2137400358915329 accum:38.04572677612305\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.21294572949409485 accum:37.90433883666992\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.20981866121292114 accum:37.347721099853516\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.21218529343605042 accum:37.76898193359375\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.21354514360427856 accum:38.01103591918945\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.21723072230815887 accum:38.66706848144531\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.21225084364414215 accum:37.7806510925293\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.21110793948173523 accum:37.577213287353516\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.2107282280921936 accum:37.50962448120117\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.21164973080158234 accum:37.67365264892578\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.2063019871711731 accum:36.72175216674805\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.21003229916095734 accum:37.38574981689453\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.20983988046646118 accum:37.351497650146484\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.21316802501678467 accum:37.94390869140625\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.21543779969215393 accum:38.34792709350586\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.21558433771133423 accum:38.3740119934082\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.21114684641361237 accum:37.584136962890625\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.21353226900100708 accum:38.00874328613281\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.21375960111618042 accum:38.04920959472656\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.20821335911750793 accum:37.06197738647461\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2094714194536209 accum:37.285911560058594\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.21717120707035065 accum:38.65647506713867\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2092737853527069 accum:37.250732421875\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.20793040096759796 accum:37.01161193847656\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.21344883739948273 accum:37.993892669677734\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2088080495595932 accum:37.16783142089844\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.21349020302295685 accum:38.00125503540039\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.2134326994419098 accum:37.99102020263672\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2122180163860321 accum:37.77480697631836\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2076781690120697 accum:36.966712951660156\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.20628447830677032 accum:36.71863555908203\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2119344025850296 accum:37.72432327270508\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.21146251261234283 accum:37.64032745361328\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.21869535744190216 accum:38.927772521972656\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.20934821665287018 accum:37.263980865478516\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.21132370829582214 accum:37.61561965942383\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.20378854870796204 accum:36.27436065673828\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.20956845581531525 accum:37.303184509277344\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.20483040809631348 accum:36.45981216430664\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.20642225444316864 accum:36.743160247802734\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.21027374267578125 accum:37.42872619628906\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.20861560106277466 accum:37.133575439453125\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.21518747508525848 accum:38.30337142944336\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.2156471163034439 accum:38.38518524169922\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.21269533038139343 accum:37.85976791381836\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.22026169300079346 accum:39.206581115722656\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2115268111228943 accum:37.651771545410156\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2131122499704361 accum:37.93397903442383\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.2202986627817154 accum:39.21316146850586\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.21411870419979095 accum:38.113128662109375\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.20637881755828857 accum:36.73542785644531\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.20975950360298157 accum:37.33719253540039\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.209245964884758 accum:37.24578094482422\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.21067506074905396 accum:37.500160217285156\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.208540141582489 accum:37.12014389038086\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.20548629760742188 accum:36.576560974121094\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.21106034517288208 accum:37.56874084472656\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.20982667803764343 accum:37.34914779663086\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.20592132210731506 accum:36.653995513916016\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.20218630135059357 accum:35.98916244506836\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.20917536318302155 accum:37.23321533203125\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.21450011432170868 accum:38.1810188293457\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.21076041460037231 accum:37.51535415649414\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.21101662516593933 accum:37.56095886230469\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.21149180829524994 accum:37.64554214477539\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.21174754202365875 accum:37.691062927246094\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.21253375709056854 accum:37.83100891113281\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2112847864627838 accum:37.60869216918945\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.20800912380218506 accum:37.0256233215332\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.21186305582523346 accum:37.71162414550781\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.2191184163093567 accum:39.00307846069336\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.20864427089691162 accum:37.13867950439453\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.21301038563251495 accum:37.91584777832031\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.2047581821680069 accum:36.446956634521484\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.20513221621513367 accum:36.51353454589844\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.20980478823184967 accum:37.345252990722656\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.20661689341068268 accum:36.77780532836914\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.21609364449977875 accum:38.46466827392578\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.21171803772449493 accum:37.68581008911133\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.21129818260669708 accum:37.61107635498047\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.20754539966583252 accum:36.94308090209961\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.21034060418605804 accum:37.44062805175781\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.21625827252864838 accum:38.49397277832031\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.21363849937915802 accum:38.027652740478516\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2096271961927414 accum:37.31364059448242\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.20971232652664185 accum:37.328792572021484\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.21323251724243164 accum:37.955387115478516\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.208210289478302 accum:37.061431884765625\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.21165001392364502 accum:37.673702239990234\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.21665404736995697 accum:38.56441879272461\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.20839381217956543 accum:37.09409713745117\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.21025435626506805 accum:37.42527389526367\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.21315798163414001 accum:37.94211959838867\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.2106826901435852 accum:37.50151824951172\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.20590800046920776 accum:36.6516227722168\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.21125715970993042 accum:37.60377502441406\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.213901087641716 accum:38.07439422607422\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2095443159341812 accum:37.29888916015625\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.2069818079471588 accum:36.8427619934082\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.21591030061244965 accum:38.43203353881836\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.20926561951637268 accum:37.2492790222168\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.20816893875598907 accum:37.05406951904297\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.21387805044651031 accum:38.07029342651367\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.2063078135251999 accum:36.7227897644043\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.20759524405002594 accum:36.95195388793945\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.20662038028240204 accum:36.77842712402344\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.21736104786396027 accum:38.69026565551758\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.20834925770759583 accum:37.08616638183594\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.21067756414413452 accum:37.500606536865234\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.2121167629957199 accum:37.75678253173828\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.20648281276226044 accum:36.75394058227539\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.2113611251115799 accum:37.62228012084961\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 155 loss avg:0.21042098104953766 accum:37.454933166503906\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.213125079870224 accum:37.93626403808594\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.21021217107772827 accum:37.41776657104492\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.20498667657375336 accum:36.48762893676758\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.20884576439857483 accum:37.17454528808594\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.21132799983024597 accum:37.61638259887695\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.2051868885755539 accum:36.52326583862305\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.2034592181444168 accum:36.21574020385742\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.21159270405769348 accum:37.66350173950195\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.20622043311595917 accum:36.707237243652344\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2139960527420044 accum:38.0912971496582\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.21071355044841766 accum:37.50701141357422\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.21427275240421295 accum:38.14054870605469\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2114824503660202 accum:37.64387512207031\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.2055255025625229 accum:36.58353805541992\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.21239270269870758 accum:37.80590057373047\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.21252325177192688 accum:37.829139709472656\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.20507866144180298 accum:36.50400161743164\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.21154135465621948 accum:37.654361724853516\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.21271386742591858 accum:37.863067626953125\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.2037804275751114 accum:36.27291488647461\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.21059319376945496 accum:37.48558807373047\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.2039824277162552 accum:36.30887222290039\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.2077917903661728 accum:36.9869384765625\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2054847925901413 accum:36.5762939453125\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.2088177353143692 accum:37.1695556640625\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.2131717950105667 accum:37.944580078125\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.20478452742099762 accum:36.45164489746094\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.20640535652637482 accum:36.74015426635742\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.20782895386219025 accum:36.993553161621094\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.2134370654821396 accum:37.991798400878906\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.20862272381782532 accum:37.13484573364258\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.20627225935459137 accum:36.716461181640625\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2027669996023178 accum:36.092525482177734\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.20680147409439087 accum:36.81066131591797\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.20895522832870483 accum:37.19403076171875\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.21374323964118958 accum:38.046295166015625\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2171993851661682 accum:38.66149139404297\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.20230235159397125 accum:36.00981903076172\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.20901595056056976 accum:37.204837799072266\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.20322641730308533 accum:36.17430114746094\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.21585555374622345 accum:38.42228698730469\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.20870734751224518 accum:37.149906158447266\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.20627184212207794 accum:36.71638870239258\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.20477700233459473 accum:36.4503059387207\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.2039230614900589 accum:36.29830551147461\n",
      "----------------------------------------\n",
      "subtract (22590, 800)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2269362509250641 accum:40.16771697998047\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.22511912882328033 accum:39.84608459472656\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.22286881506443024 accum:39.44778060913086\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.2244146168231964 accum:39.72138595581055\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.2240709364414215 accum:39.66055679321289\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.22984880208969116 accum:40.6832389831543\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.22894293069839478 accum:40.52289962768555\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.2268006056547165 accum:40.143707275390625\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.2224012315273285 accum:39.36501693725586\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.22325541079044342 accum:39.51620864868164\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.22067739069461823 accum:39.059898376464844\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.22451971471309662 accum:39.739990234375\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.22987639904022217 accum:40.6881217956543\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.22581404447555542 accum:39.969085693359375\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.22817997634410858 accum:40.387855529785156\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.22459430992603302 accum:39.75319290161133\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.22133396565914154 accum:39.17611312866211\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.22119300067424774 accum:39.151161193847656\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.22192488610744476 accum:39.280704498291016\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.22537171840667725 accum:39.89079284667969\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.22581978142261505 accum:39.97010040283203\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.2230898141860962 accum:39.48689651489258\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.2185104936361313 accum:38.67635726928711\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.22053425014019012 accum:39.03456115722656\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.22143113613128662 accum:39.19330978393555\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.22030150890350342 accum:38.99336624145508\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.21697379648685455 accum:38.404361724853516\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.21645139157772064 accum:38.31189727783203\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.22245310246944427 accum:39.37419891357422\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.22340664267539978 accum:39.54297637939453\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.21830913424491882 accum:38.640716552734375\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.21866045892238617 accum:38.70289993286133\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2206835150718689 accum:39.06098175048828\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.22133061289787292 accum:39.17551803588867\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.22321000695228577 accum:39.50817108154297\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.22223909199237823 accum:39.33631896972656\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.22665072977542877 accum:40.11717987060547\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.22256411612033844 accum:39.39384841918945\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.22496089339256287 accum:39.818077087402344\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.21716377139091492 accum:38.43798828125\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.22083494067192078 accum:39.08778381347656\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.23041196167469025 accum:40.78291702270508\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.21994061768054962 accum:38.92948913574219\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.22597253322601318 accum:39.99713897705078\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2245377004146576 accum:39.74317169189453\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.22170598804950714 accum:39.24195861816406\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.2187899947166443 accum:38.725830078125\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.22043180465698242 accum:39.01642990112305\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.2246815264225006 accum:39.76863098144531\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.22120361030101776 accum:39.153038024902344\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.22587108612060547 accum:39.979183197021484\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.22894321382045746 accum:40.52294921875\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.22260507941246033 accum:39.401100158691406\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.2191186398267746 accum:38.784000396728516\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2171565145254135 accum:38.436702728271484\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.22301293909549713 accum:39.473289489746094\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.21596907079219818 accum:38.226524353027344\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.21870385110378265 accum:38.7105827331543\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.22453944385051727 accum:39.74348068237305\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.22433049976825714 accum:39.70649719238281\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.22132804989814758 accum:39.17506408691406\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.21842460334300995 accum:38.661155700683594\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.21930275857448578 accum:38.81658935546875\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2197619080543518 accum:38.897857666015625\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.21905194222927094 accum:38.772193908691406\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.21827562153339386 accum:38.63478469848633\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.21925252676010132 accum:38.80769729614258\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.2181900441646576 accum:38.61963653564453\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.21776770055294037 accum:38.544883728027344\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.22259753942489624 accum:39.39976501464844\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.2204056680202484 accum:39.011802673339844\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.22117874026298523 accum:39.14863586425781\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2219654619693756 accum:39.28788757324219\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.22256948053836823 accum:39.394798278808594\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.22324636578559875 accum:39.51460647583008\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.22078146040439606 accum:39.07831954956055\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.2212950438261032 accum:39.16922378540039\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.22277246415615082 accum:39.43072509765625\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.21750979125499725 accum:38.49923324584961\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.22160924971103668 accum:39.22483825683594\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.22043409943580627 accum:39.0168342590332\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.2262735366821289 accum:40.0504150390625\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.22499993443489075 accum:39.824989318847656\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.22448572516441345 accum:39.73397445678711\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.22526714205741882 accum:39.872283935546875\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.22533556818962097 accum:39.884395599365234\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.22575658559799194 accum:39.95891571044922\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.22633101046085358 accum:40.06058883666992\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.22503767907619476 accum:39.831668853759766\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2311326414346695 accum:40.91047668457031\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.22413095831871033 accum:39.671180725097656\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.22175531089305878 accum:39.25069046020508\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.21668365597724915 accum:38.35300827026367\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.21748128533363342 accum:38.49418640136719\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.2232537418603897 accum:39.51591110229492\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.21683967113494873 accum:38.38062286376953\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.2210751324892044 accum:39.13029861450195\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.21764421463012695 accum:38.52302551269531\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.2191130369901657 accum:38.78300857543945\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.22333049774169922 accum:39.52949905395508\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.21811938285827637 accum:38.60713195800781\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.21851937472820282 accum:38.67792892456055\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.21696051955223083 accum:38.40201187133789\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.21919254958629608 accum:38.797080993652344\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.22152073681354523 accum:39.209171295166016\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2222200185060501 accum:39.332942962646484\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.22525256872177124 accum:39.86970520019531\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2225203663110733 accum:39.386104583740234\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.21771878004074097 accum:38.536224365234375\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 109 loss avg:0.2252580225467682 accum:39.870670318603516\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.21782264113426208 accum:38.55460739135742\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.21765057742595673 accum:38.52415084838867\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.21377429366111755 accum:37.838050842285156\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.21856458485126495 accum:38.68593215942383\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2243329882621765 accum:39.706939697265625\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.2199409008026123 accum:38.92953872680664\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.21955667436122894 accum:38.86153030395508\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.21702134609222412 accum:38.412776947021484\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.21657182276248932 accum:38.333213806152344\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.22610662877559662 accum:40.0208740234375\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.21933841705322266 accum:38.822898864746094\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.21698519587516785 accum:38.40637969970703\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2214517593383789 accum:39.19696044921875\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.22280994057655334 accum:39.43735885620117\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.2183760106563568 accum:38.65255355834961\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.2185896337032318 accum:38.690364837646484\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.21549655497074127 accum:38.14289093017578\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2190326303243637 accum:38.768775939941406\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.21966412663459778 accum:38.880550384521484\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.22157129645347595 accum:39.21812057495117\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.22330541908740997 accum:39.52505874633789\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.2255249172449112 accum:39.917911529541016\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.22303254902362823 accum:39.47676086425781\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.21997283399105072 accum:38.9351921081543\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.21722263097763062 accum:38.44840621948242\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.21577438712120056 accum:38.19206619262695\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.21984487771987915 accum:38.91254425048828\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.22206361591815948 accum:39.305259704589844\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.21953944861888885 accum:38.858482360839844\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.21938276290893555 accum:38.83074951171875\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2193697988986969 accum:38.828453063964844\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.21585141122341156 accum:38.2056999206543\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.21441704034805298 accum:37.95181655883789\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2174016833305359 accum:38.480098724365234\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.21611888706684113 accum:38.25304412841797\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.21657252311706543 accum:38.333335876464844\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.21841347217559814 accum:38.659183502197266\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.21496735513210297 accum:38.04922103881836\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.21525666117668152 accum:38.10042953491211\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.21888485550880432 accum:38.742618560791016\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.2191493809223175 accum:38.7894401550293\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.22751568257808685 accum:40.2702751159668\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.22038932144641876 accum:39.0089111328125\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.21540355682373047 accum:38.12643051147461\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.21604932844638824 accum:38.24073028564453\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.2171974778175354 accum:38.44395446777344\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.21806104481220245 accum:38.596805572509766\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.2198019027709961 accum:38.904937744140625\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.21840986609458923 accum:38.658546447753906\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2186337113380432 accum:38.69816589355469\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.2236248105764389 accum:39.58159255981445\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.21828977763652802 accum:38.637290954589844\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.2221938520669937 accum:39.328311920166016\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2221374362707138 accum:39.31832504272461\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.21789170801639557 accum:38.56683349609375\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2205752730369568 accum:39.04182434082031\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2178128957748413 accum:38.55288314819336\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.21834515035152435 accum:38.647090911865234\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.21613796055316925 accum:38.25642013549805\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.21746981143951416 accum:38.492156982421875\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.21913214027881622 accum:38.7863883972168\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.21633906662464142 accum:38.292015075683594\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.21741566061973572 accum:38.48257064819336\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.21891412138938904 accum:38.747798919677734\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2207123041152954 accum:39.066078186035156\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.22405917942523956 accum:39.65847396850586\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.2163884937763214 accum:38.30076217651367\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.21641480922698975 accum:38.305419921875\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.21667124330997467 accum:38.35081100463867\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.21809962391853333 accum:38.603633880615234\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.21751756966114044 accum:38.5006103515625\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.21636049449443817 accum:38.295806884765625\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.2178070843219757 accum:38.55185317993164\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.21724803745746613 accum:38.452903747558594\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.219645693898201 accum:38.877288818359375\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.21647875010967255 accum:38.31673812866211\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.2254888266324997 accum:39.911521911621094\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.2176201343536377 accum:38.51876449584961\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.21511594951152802 accum:38.075523376464844\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.21779626607894897 accum:38.5499382019043\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.2205493301153183 accum:39.0372314453125\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2194218635559082 accum:38.837669372558594\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.21648907661437988 accum:38.318565368652344\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.21694821119308472 accum:38.39983367919922\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.21307584643363953 accum:37.71442413330078\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.21431751549243927 accum:37.934200286865234\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.2186901569366455 accum:38.70815658569336\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.21281173825263977 accum:37.66767883300781\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.21324650943279266 accum:37.744632720947266\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.2188548445701599 accum:38.737308502197266\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.21253715455532074 accum:37.619075775146484\n",
      "----------------------------------------\n",
      "multiply (22678, 800)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.19769930839538574 accum:35.19047546386719\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.19418519735336304 accum:34.564964294433594\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.20583322644233704 accum:36.63831329345703\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.19734826683998108 accum:35.12799072265625\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.19920891523361206 accum:35.45918655395508\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.20205405354499817 accum:35.96562194824219\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.19362372159957886 accum:34.465023040771484\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.18937452137470245 accum:33.70866394042969\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.2044503092765808 accum:36.392154693603516\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.19849663972854614 accum:35.332401275634766\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.200590118765831 accum:35.705039978027344\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.20263968408107758 accum:36.069862365722656\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.20040544867515564 accum:35.67216873168945\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.20090122520923615 accum:35.76041793823242\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.19799500703811646 accum:35.24311065673828\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.19865530729293823 accum:35.36064529418945\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.19383400678634644 accum:34.5024528503418\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.19203083217144012 accum:34.181488037109375\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.19683872163295746 accum:35.03729248046875\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.1987525075674057 accum:35.37794494628906\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.19731886684894562 accum:35.12275695800781\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.20192816853523254 accum:35.943214416503906\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.20509161055088043 accum:36.50630569458008\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.19306764006614685 accum:34.36603927612305\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.19657941162586212 accum:34.99113464355469\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.20369817316532135 accum:36.25827407836914\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.2017408013343811 accum:35.90986251831055\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.20431531965732574 accum:36.368125915527344\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.20077529549598694 accum:35.73800277709961\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.20057837665081024 accum:35.70294952392578\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.19862432777881622 accum:35.35512924194336\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.20306514203548431 accum:36.14559555053711\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.19983439147472382 accum:35.57052230834961\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.19851826131343842 accum:35.33625030517578\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.20280709862709045 accum:36.09966278076172\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.1975359320640564 accum:35.16139602661133\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.20360007882118225 accum:36.240814208984375\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.19678975641727448 accum:35.0285758972168\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.19577519595623016 accum:34.847984313964844\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.1993568241596222 accum:35.48551559448242\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.20385345816612244 accum:36.28591537475586\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.19373100996017456 accum:34.4841194152832\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.1954302042722702 accum:34.78657531738281\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.20370037853717804 accum:36.2586669921875\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.19304554164409637 accum:34.36210632324219\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.19510963559150696 accum:34.729515075683594\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.1945490688085556 accum:34.62973403930664\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.18484745919704437 accum:32.90284729003906\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.19484102725982666 accum:34.68170166015625\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.19896286725997925 accum:35.41539001464844\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.19469065964221954 accum:34.654937744140625\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.2003909945487976 accum:35.66959762573242\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.2098139524459839 accum:37.34688186645508\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.18964853882789612 accum:33.75743865966797\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.19242657721042633 accum:34.251930236816406\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.18826326727867126 accum:33.510860443115234\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.19079595804214478 accum:33.9616813659668\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.19910681247711182 accum:35.44101333618164\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.1941460222005844 accum:34.55799102783203\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.1943815052509308 accum:34.59990692138672\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.1973562091588974 accum:35.1294059753418\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.18785521388053894 accum:33.438228607177734\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.19193434715270996 accum:34.16431427001953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.18801359832286835 accum:33.4664192199707\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.19710594415664673 accum:35.08485794067383\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.19667766988277435 accum:35.00862503051758\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.1955651044845581 accum:34.81058883666992\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.19574595987796783 accum:34.84278106689453\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.19132989645004272 accum:34.05672073364258\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.19160956144332886 accum:34.106502532958984\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.19404421746730804 accum:34.53987121582031\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.19944514334201813 accum:35.50123596191406\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.1929689347743988 accum:34.34846878051758\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.19535338878631592 accum:34.77290344238281\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.19848161935806274 accum:35.32972717285156\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.20361411571502686 accum:36.24331283569336\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.19719856977462769 accum:35.10134506225586\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.1922653764486313 accum:34.223236083984375\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.19466659426689148 accum:34.65065383911133\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.19956327974796295 accum:35.52226257324219\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.19848474860191345 accum:35.330284118652344\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.2008097767829895 accum:35.744140625\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.20748308300971985 accum:36.93198776245117\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.20266865193843842 accum:36.07501983642578\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.19639606773853302 accum:34.958499908447266\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.19211505353450775 accum:34.19647979736328\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.18812552094459534 accum:33.48634338378906\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.19509290158748627 accum:34.72653579711914\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.20185181498527527 accum:35.929622650146484\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.1938314139842987 accum:34.501991271972656\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.19530925154685974 accum:34.765045166015625\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.19325080513954163 accum:34.398643493652344\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.20451588928699493 accum:36.40382766723633\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.1989898979663849 accum:35.42020034790039\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.1948939710855484 accum:34.69112777709961\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.18896645307540894 accum:33.63602828979492\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.18823842704296112 accum:33.506439208984375\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.18871702253818512 accum:33.59162902832031\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.1986522674560547 accum:35.360103607177734\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.19596922397613525 accum:34.88252258300781\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.20258678495883942 accum:36.060447692871094\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.19370245933532715 accum:34.47903823852539\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.18937236070632935 accum:33.70827865600586\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.1981041133403778 accum:35.26253128051758\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.19804605841636658 accum:35.252197265625\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.19559551775455475 accum:34.816001892089844\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.19937415421009064 accum:35.48859786987305\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.19515253603458405 accum:34.737152099609375\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.20142623782157898 accum:35.8538703918457\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.1924864500761032 accum:34.26258850097656\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.19221502542495728 accum:34.21427536010742\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.18387719988822937 accum:32.730140686035156\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.19014401733875275 accum:33.84563446044922\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.19362807273864746 accum:34.465797424316406\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.1949838548898697 accum:34.70712661743164\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.1899876594543457 accum:33.81780242919922\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.19211743772029877 accum:34.196903228759766\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.20185180008411407 accum:35.92961883544922\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.19706040620803833 accum:35.076751708984375\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.19221971929073334 accum:34.215110778808594\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.1952541172504425 accum:34.75523376464844\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.1958554983139038 accum:34.86227798461914\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.19668662548065186 accum:35.01021957397461\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.19493582844734192 accum:34.698577880859375\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.19425317645072937 accum:34.577064514160156\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.18796934187412262 accum:33.45854187011719\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.18910883367061615 accum:33.661373138427734\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.19416682422161102 accum:34.56169509887695\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.194205641746521 accum:34.568603515625\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.19898246228694916 accum:35.41887664794922\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.19474488496780396 accum:34.664588928222656\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.20328070223331451 accum:36.183963775634766\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.1955663114786148 accum:34.8108024597168\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.2064293622970581 accum:36.74442672729492\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.1978275626897812 accum:35.21330642700195\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.19363339245319366 accum:34.46674346923828\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.19776193797588348 accum:35.20162582397461\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.19401565194129944 accum:34.534786224365234\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.20328204333782196 accum:36.1842041015625\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.19926021993160248 accum:35.468318939208984\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.19674052298069 accum:35.019813537597656\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.1972116231918335 accum:35.103668212890625\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2005813717842102 accum:35.70348358154297\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.20030808448791504 accum:35.65483856201172\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.19160959124565125 accum:34.10650634765625\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.1862827092409134 accum:33.158321380615234\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.19309557974338531 accum:34.37101364135742\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.2060156911611557 accum:36.67079162597656\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.20088976621627808 accum:35.75837707519531\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.20194052159786224 accum:35.945411682128906\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.19626469910144806 accum:34.935115814208984\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.19641204178333282 accum:34.961341857910156\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.20327375829219818 accum:36.1827278137207\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.19836406409740448 accum:35.30880355834961\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.1962055116891861 accum:34.92457962036133\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.18889684975147247 accum:33.62363815307617\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.1907169073820114 accum:33.947608947753906\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.19219309091567993 accum:34.21036911010742\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.18656516075134277 accum:33.20859909057617\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.19450055062770844 accum:34.621097564697266\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.1923806518316269 accum:34.24375534057617\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.1961514800786972 accum:34.91496276855469\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.20574553310871124 accum:36.622703552246094\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.20739568769931793 accum:36.91643142700195\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.2041652351617813 accum:36.34141159057617\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.1935635507106781 accum:34.45431137084961\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2015731930732727 accum:35.880027770996094\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.20531073212623596 accum:36.545310974121094\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.20794937014579773 accum:37.01498794555664\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.1942085474729538 accum:34.569122314453125\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.19307751953601837 accum:34.3677978515625\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.19525498151779175 accum:34.75538635253906\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.1972903162240982 accum:35.11767578125\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.20047593116760254 accum:35.684715270996094\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2001032680273056 accum:35.61838150024414\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.20672373473644257 accum:36.79682540893555\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.19738353788852692 accum:35.13426971435547\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.1987413614988327 accum:35.37596130371094\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.1930929720401764 accum:34.370548248291016\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.19081945717334747 accum:33.96586227416992\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.1950174868106842 accum:34.713111877441406\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.20951060950756073 accum:37.29288864135742\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.19852975010871887 accum:35.338294982910156\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.18948683142662048 accum:33.72865676879883\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.18672315776348114 accum:33.23672103881836\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.1931251436471939 accum:34.37627410888672\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.20376209914684296 accum:36.2696533203125\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.20246389508247375 accum:36.03857421875\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.20255796611309052 accum:36.05531692504883\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2129567414522171 accum:37.90629959106445\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.21121779084205627 accum:37.59676742553711\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2059403955936432 accum:36.65739059448242\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2051607072353363 accum:36.51860427856445\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.20230627059936523 accum:36.01051712036133\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.20458759367465973 accum:36.41659164428711\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.20229755342006683 accum:36.00896453857422\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.20313110947608948 accum:36.1573371887207\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.20050863921642303 accum:35.69053649902344\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.1959618777036667 accum:34.8812141418457\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.20013514161109924 accum:35.624053955078125\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.20907220244407654 accum:37.21485137939453\n",
      "----------------------------------------\n",
      "divide (14746, 800)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.13738003373146057 accum:15.936083793640137\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.13451161980628967 accum:15.603348731994629\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.13479220867156982 accum:15.635895729064941\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.13607342541217804 accum:15.784517288208008\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.1362774670124054 accum:15.808185577392578\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.1372961848974228 accum:15.926358222961426\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.1329760104417801 accum:15.425217628479004\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.13472513854503632 accum:15.628116607666016\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.1396155208349228 accum:16.19540023803711\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.13570371270179749 accum:15.741631507873535\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.14236687123775482 accum:16.514556884765625\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.13657799363136292 accum:15.843048095703125\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.13452628254890442 accum:15.605048179626465\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.1343366503715515 accum:15.583051681518555\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.13688434660434723 accum:15.878583908081055\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.13671736419200897 accum:15.859214782714844\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 loss avg:0.1415305733680725 accum:16.41754722595215\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.14350362122058868 accum:16.646419525146484\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.13800352811813354 accum:16.00840950012207\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.1357932835817337 accum:15.752021789550781\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.1371433436870575 accum:15.9086275100708\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.13807249069213867 accum:16.016408920288086\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.12841302156448364 accum:14.89591121673584\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.13504788279533386 accum:15.665555000305176\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.1318768411874771 accum:15.297714233398438\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.1375596970319748 accum:15.956924438476562\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.14412446320056915 accum:16.71843719482422\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.13495582342147827 accum:15.654874801635742\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.1339353322982788 accum:15.536498069763184\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.13475258648395538 accum:15.63129997253418\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.13015006482601166 accum:15.097408294677734\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.1362958550453186 accum:15.810318946838379\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.13702844083309174 accum:15.895299911499023\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.14310713112354279 accum:16.600427627563477\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.14420545101165771 accum:16.727832794189453\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.14074917137622833 accum:16.326904296875\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.13877636194229126 accum:16.098058700561523\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.13840416073799133 accum:16.054882049560547\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.13841132819652557 accum:16.055713653564453\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.13907799124717712 accum:16.133047103881836\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.1340344399213791 accum:15.547994613647461\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.1361614614725113 accum:15.794730186462402\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.13257190585136414 accum:15.378340721130371\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.13162562251091003 accum:15.268572807312012\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.13289496302604675 accum:15.415816307067871\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.13246466219425201 accum:15.365900039672852\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.13843569159507751 accum:16.05854034423828\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.1341378092765808 accum:15.559985160827637\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.132681742310524 accum:15.391081809997559\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.13169360160827637 accum:15.276457786560059\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.13391101360321045 accum:15.53367805480957\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.13803932070732117 accum:16.012561798095703\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.13538579642772675 accum:15.704752922058105\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.13953620195388794 accum:16.186199188232422\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.13374333083629608 accum:15.514225959777832\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.13248996436595917 accum:15.36883544921875\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.13524989783763885 accum:15.688987731933594\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.13495899736881256 accum:15.655242919921875\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.13229283690452576 accum:15.345969200134277\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.13466504216194153 accum:15.62114429473877\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.1346326470375061 accum:15.617387771606445\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.13177049160003662 accum:15.28537654876709\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.13183708488941193 accum:15.293102264404297\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.1345367580652237 accum:15.606263160705566\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.13376472890377045 accum:15.516709327697754\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.13867318630218506 accum:16.086090087890625\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.13747280836105347 accum:15.946845054626465\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.13624568283557892 accum:15.804499626159668\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.13673484325408936 accum:15.861241340637207\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.14315642416477203 accum:16.60614585876465\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.13646645843982697 accum:15.830109596252441\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.1342056840658188 accum:15.567858695983887\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.13036580383777618 accum:15.122432708740234\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.12784983217716217 accum:14.830580711364746\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.13287927210330963 accum:15.413994789123535\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.136201873421669 accum:15.799416542053223\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.13800139725208282 accum:16.008161544799805\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.13735873997211456 accum:15.933614730834961\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.13675686717033386 accum:15.863797187805176\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.138511061668396 accum:16.067283630371094\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.1325473040342331 accum:15.375487327575684\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.13184447586536407 accum:15.29395866394043\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.13136498630046844 accum:15.238338470458984\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.13976861536502838 accum:16.213159561157227\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.13338296115398407 accum:15.472423553466797\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.12941254675388336 accum:15.011856079101562\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.13463453948497772 accum:15.617606163024902\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.13317199051380157 accum:15.447951316833496\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.13368460536003113 accum:15.507414817810059\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.1361018568277359 accum:15.78781509399414\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.13395562767982483 accum:15.53885269165039\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.13141502439975739 accum:15.24414348602295\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.13064786791801453 accum:15.155152320861816\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.1368725448846817 accum:15.877215385437012\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.13093653321266174 accum:15.188638687133789\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.12678027153015137 accum:14.706511497497559\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.132997527718544 accum:15.427713394165039\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.1269424557685852 accum:14.725324630737305\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.1402289718389511 accum:16.26656150817871\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.1344320923089981 accum:15.594122886657715\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.13466832041740417 accum:15.621525764465332\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.13444361090660095 accum:15.595458984375\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.13589587807655334 accum:15.763922691345215\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.13070866465568542 accum:15.16220474243164\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.13174757361412048 accum:15.282718658447266\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.1375683695077896 accum:15.957930564880371\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.13274841010570526 accum:15.398815155029297\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.1362345665693283 accum:15.80320930480957\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.13131484389305115 accum:15.232522010803223\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.13140122592449188 accum:15.242542266845703\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.12766103446483612 accum:14.808679580688477\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.13132146000862122 accum:15.233288764953613\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.12824322283267975 accum:14.876214027404785\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.13558058440685272 accum:15.727347373962402\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.13052992522716522 accum:15.141470909118652\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.12743504345417023 accum:14.782465934753418\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.1326972097158432 accum:15.392876625061035\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.13581153750419617 accum:15.754138946533203\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.13186316192150116 accum:15.296126365661621\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.133652001619339 accum:15.503631591796875\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.13671629130840302 accum:15.859089851379395\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.13662821054458618 accum:15.848873138427734\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.1414906531572342 accum:16.41291618347168\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.1350603550672531 accum:15.667001724243164\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.13563698530197144 accum:15.73388957977295\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.1384229063987732 accum:16.057056427001953\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.13563628494739532 accum:15.733809471130371\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.1369086503982544 accum:15.881403923034668\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.13675525784492493 accum:15.863609313964844\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.13329052925109863 accum:15.461701393127441\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.13591426610946655 accum:15.766054153442383\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.13340066373348236 accum:15.474477767944336\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.135644793510437 accum:15.734796524047852\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.13435043394565582 accum:15.584650993347168\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.12965889275074005 accum:15.04043197631836\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.12845677137374878 accum:14.900984764099121\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.13346898555755615 accum:15.482401847839355\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.1305810511112213 accum:15.1474027633667\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.13213281333446503 accum:15.32740592956543\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.13117539882659912 accum:15.216346740722656\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.13599787652492523 accum:15.775753021240234\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.1354522705078125 accum:15.71246337890625\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.13913977146148682 accum:16.140213012695312\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.1325656920671463 accum:15.377619743347168\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.13574494421482086 accum:15.746413230895996\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.14005474746227264 accum:16.24635124206543\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.1404431313276291 accum:16.29140281677246\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.13833124935626984 accum:16.046424865722656\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.13515350222587585 accum:15.677806854248047\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.12818707525730133 accum:14.869701385498047\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.13326600193977356 accum:15.458855628967285\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.13431686162948608 accum:15.580755233764648\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.13185039162635803 accum:15.294645309448242\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.12803198397159576 accum:14.851709365844727\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.13589054346084595 accum:15.76330280303955\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.14023979008197784 accum:16.2678165435791\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.13283203542232513 accum:15.408515930175781\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.13579866290092468 accum:15.752644538879395\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.13490383327007294 accum:15.648844718933105\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.1349320262670517 accum:15.652115821838379\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.13911980390548706 accum:16.137897491455078\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.1400546133518219 accum:16.246335983276367\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.135459765791893 accum:15.713333129882812\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.1376456767320633 accum:15.966897964477539\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.1347939521074295 accum:15.636098861694336\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.1351650208234787 accum:15.679142951965332\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.13223066926002502 accum:15.338757514953613\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.13106270134449005 accum:15.203272819519043\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.13058166205883026 accum:15.147473335266113\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.1349416971206665 accum:15.653237342834473\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.13341525197029114 accum:15.47616958618164\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.12743715941905975 accum:14.782710075378418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.12542618811130524 accum:14.549437522888184\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.13025692105293274 accum:15.109803199768066\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.13078942894935608 accum:15.171573638916016\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.1333627998828888 accum:15.470085144042969\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.13259413838386536 accum:15.38092041015625\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.12772111594676971 accum:14.81564998626709\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.12946383655071259 accum:15.017805099487305\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.1273552030324936 accum:14.77320384979248\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.1290569007396698 accum:14.970600128173828\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.12986783683300018 accum:15.064669609069824\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.13021288812160492 accum:15.104694366455078\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.12876950204372406 accum:14.937262535095215\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.1359235644340515 accum:15.767132759094238\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.14291436970233917 accum:16.578067779541016\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.13588140904903412 accum:15.76224422454834\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.1320384293794632 accum:15.316457748413086\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.1352842003107071 accum:15.69296646118164\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.13241374492645264 accum:15.359994888305664\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.13307245075702667 accum:15.436405181884766\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.13621613383293152 accum:15.801071166992188\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.1324174702167511 accum:15.36042594909668\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.1312934309244156 accum:15.230037689208984\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.13408583402633667 accum:15.553956031799316\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.13228340446949005 accum:15.344874382019043\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.12808848917484283 accum:14.858263969421387\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.12836545705795288 accum:14.890392303466797\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.12849821150302887 accum:14.905793190002441\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.1293460577726364 accum:15.004142761230469\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.12960973381996155 accum:15.03472900390625\n",
      "----------------------------------------\n",
      "log (3310, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.21193961799144745 accum:5.510429859161377\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.20817165076732635 accum:5.4124627113342285\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.20651334524154663 accum:5.369346618652344\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.21145763993263245 accum:5.497898578643799\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.21577784419059753 accum:5.610223770141602\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2135920226573944 accum:5.55339241027832\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.21091033518314362 accum:5.483668327331543\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.2055944949388504 accum:5.345456600189209\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.21420276165008545 accum:5.569271564483643\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.21622994542121887 accum:5.621978282928467\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.220201313495636 accum:5.725234031677246\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.22712452709674835 accum:5.905237674713135\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2192724347114563 accum:5.701083183288574\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.21124477684497833 accum:5.492363929748535\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.21019653975963593 accum:5.465109825134277\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.2216002494096756 accum:5.761606216430664\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.21890702843666077 accum:5.691582679748535\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.21714378893375397 accum:5.645738124847412\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.20963720977306366 accum:5.450567245483398\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.2209022343158722 accum:5.743457794189453\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.21621716022491455 accum:5.621645927429199\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.2129305750131607 accum:5.536194801330566\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.212255597114563 accum:5.518645286560059\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.20541337132453918 accum:5.340747356414795\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.2114303857088089 accum:5.497189998626709\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.21258693933486938 accum:5.5272603034973145\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.21918730437755585 accum:5.698869705200195\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.20952819287776947 accum:5.447732925415039\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.20886091887950897 accum:5.430383682250977\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.22411660850048065 accum:5.82703161239624\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.21152670681476593 accum:5.499694347381592\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.2103656381368637 accum:5.46950626373291\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2118983119726181 accum:5.5093560218811035\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.213369682431221 accum:5.547611713409424\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.21659331023693085 accum:5.631425857543945\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.21333914995193481 accum:5.546817779541016\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.21578963100910187 accum:5.610530376434326\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.20512768626213074 accum:5.333319664001465\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.2144804298877716 accum:5.576490879058838\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.22334930300712585 accum:5.807081699371338\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.21671782433986664 accum:5.634663105010986\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.21793852746486664 accum:5.666401386260986\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.22067998349666595 accum:5.737679481506348\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.21030239760875702 accum:5.467862129211426\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2134270966053009 accum:5.5491042137146\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.22251814603805542 accum:5.785471439361572\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.22282887995243073 accum:5.793550491333008\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.21972118318080902 accum:5.712750434875488\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.21781082451343536 accum:5.663081169128418\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.2190326601266861 accum:5.694849014282227\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.22002330422401428 accum:5.720605850219727\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.21813833713531494 accum:5.671596527099609\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.21777649223804474 accum:5.662188529968262\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.22129924595355988 accum:5.753780364990234\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.21626685559749603 accum:5.62293815612793\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.20884954929351807 accum:5.430088043212891\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2106993943452835 accum:5.478184223175049\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.2033335566520691 accum:5.286672115325928\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.2161438912153244 accum:5.619740962982178\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.2085728794336319 accum:5.422894477844238\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.2034069150686264 accum:5.28857946395874\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.19848057627677917 accum:5.160494804382324\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2168157398700714 accum:5.637208938598633\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2191115766763687 accum:5.696900844573975\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.21210306882858276 accum:5.514679431915283\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.212894469499588 accum:5.5352559089660645\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.21291738748550415 accum:5.535851955413818\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.21681608259677887 accum:5.637217998504639\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2080950289964676 accum:5.410470485687256\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.21121437847614288 accum:5.491573810577393\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.21150900423526764 accum:5.499233722686768\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2113342583179474 accum:5.494690418243408\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2082044631242752 accum:5.413315773010254\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.21567286550998688 accum:5.607494354248047\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.21513131260871887 accum:5.593413829803467\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.20711171627044678 accum:5.384904384613037\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.21096192300319672 accum:5.485009670257568\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.20107997953891754 accum:5.228079319000244\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.2122017741203308 accum:5.517245769500732\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.2143177092075348 accum:5.57226037979126\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.20587226748466492 accum:5.3526787757873535\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.20963652431964874 accum:5.450549602508545\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.218791201710701 accum:5.688570976257324\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.2075633406639099 accum:5.396646499633789\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.20871463418006897 accum:5.426580429077148\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.20724955201148987 accum:5.388488292694092\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.21609504520893097 accum:5.618471145629883\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.21345718204975128 accum:5.549886703491211\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.21128028631210327 accum:5.493287086486816\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.21572040021419525 accum:5.608730316162109\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.21462994813919067 accum:5.580378532409668\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.22200772166252136 accum:5.772200584411621\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.21168111264705658 accum:5.503708839416504\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.2104782909154892 accum:5.472435474395752\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.2083623707294464 accum:5.417421340942383\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.21344059705734253 accum:5.549455165863037\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.20778900384902954 accum:5.4025139808654785\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.19923990964889526 accum:5.180237293243408\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.21146947145462036 accum:5.49820613861084\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.2209324985742569 accum:5.744244575500488\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.21911314129829407 accum:5.696941375732422\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.21808359026908875 accum:5.670173168182373\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.21862581372261047 accum:5.684270858764648\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2123480588197708 accum:5.521049499511719\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.21479307115077972 accum:5.584619522094727\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2145928144454956 accum:5.579412937164307\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.21521848440170288 accum:5.595680236816406\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2155417948961258 accum:5.604086399078369\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.22013546526432037 accum:5.723521709442139\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.2109292894601822 accum:5.484161376953125\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.22635318338871002 accum:5.8851823806762695\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.21858049929141998 accum:5.6830925941467285\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.214689239859581 accum:5.581920146942139\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.22072620689868927 accum:5.7388811111450195\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.20867060124874115 accum:5.425435543060303\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.2087530791759491 accum:5.427579879760742\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.22077210247516632 accum:5.740074634552002\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.20699910819530487 accum:5.38197660446167\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.21325139701366425 accum:5.544536113739014\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.21466004848480225 accum:5.581161022186279\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.20375460386276245 accum:5.297619342803955\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.223243847489357 accum:5.80433988571167\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.21292097866535187 accum:5.535945415496826\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.21320496499538422 accum:5.543328762054443\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.20133307576179504 accum:5.234659671783447\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.2045755833387375 accum:5.318964958190918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.2083653211593628 accum:5.4174981117248535\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.20781558752059937 accum:5.403204917907715\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.20401819050312042 accum:5.304472923278809\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.21434669196605682 accum:5.573013782501221\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.2126951962709427 accum:5.5300750732421875\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.20555773377418518 accum:5.34450101852417\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.2038154900074005 accum:5.2992024421691895\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.20746496319770813 accum:5.3940887451171875\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2138417363166809 accum:5.559885025024414\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.20099419355392456 accum:5.22584867477417\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.2142421305179596 accum:5.570295333862305\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.2030082792043686 accum:5.278214931488037\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.2084714025259018 accum:5.4202561378479\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.21139927208423615 accum:5.496380805969238\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2267775982618332 accum:5.896217346191406\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.20516736805438995 accum:5.334351539611816\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.20138190686702728 accum:5.235929489135742\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2075408548116684 accum:5.396061897277832\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.21156059205532074 accum:5.500575065612793\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.20839622616767883 accum:5.418301582336426\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.20334193110466003 accum:5.286890029907227\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.21857371926307678 accum:5.682916641235352\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.22332289814949036 accum:5.806395053863525\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.21356512606143951 accum:5.552692890167236\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.21264660358428955 accum:5.528811454772949\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.2126852422952652 accum:5.529816150665283\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.20628559589385986 accum:5.363425254821777\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.20917624235153198 accum:5.438581943511963\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.20496931672096252 accum:5.329202175140381\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.20339742302894592 accum:5.288332939147949\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.2130720168352127 accum:5.539872169494629\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.20930851995944977 accum:5.442021369934082\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2015886902809143 accum:5.241305828094482\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2092297375202179 accum:5.439972877502441\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.21549102663993835 accum:5.602766513824463\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.22018873691558838 accum:5.724906921386719\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.2221881002187729 accum:5.776890277862549\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2073574811220169 accum:5.391294479370117\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.20864008367061615 accum:5.424642086029053\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2091001570224762 accum:5.436604022979736\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.20923927426338196 accum:5.440220832824707\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.2042936384677887 accum:5.311634540557861\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.21207544207572937 accum:5.513961315155029\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.21745485067367554 accum:5.653825759887695\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.2102944403886795 accum:5.467655181884766\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.22262874245643616 accum:5.788347244262695\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.2188500463962555 accum:5.690101146697998\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.21329645812511444 accum:5.545707702636719\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.20833885669708252 accum:5.416810035705566\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.20831120014190674 accum:5.416090965270996\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.21392324566841125 accum:5.562004089355469\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.21288760006427765 accum:5.5350775718688965\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.21335241198539734 accum:5.5471625328063965\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2119387686252594 accum:5.5104079246521\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.21169023215770721 accum:5.503945827484131\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.20385617017745972 accum:5.300260066986084\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.21125641465187073 accum:5.492666721343994\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.20411375164985657 accum:5.306957244873047\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.21470032632350922 accum:5.582208156585693\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.207137793302536 accum:5.385582447052002\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.20349422097206116 accum:5.290849685668945\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.20564785599708557 accum:5.34684419631958\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.20161950588226318 accum:5.242106914520264\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.20407281816005707 accum:5.3058929443359375\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.21023648977279663 accum:5.466148376464844\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2018919587135315 accum:5.249190807342529\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.19917729496955872 accum:5.178609371185303\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.20566561818122864 accum:5.347305774688721\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2018594890832901 accum:5.248346328735352\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.20106245577335358 accum:5.227623462677002\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.20645593106746674 accum:5.367854118347168\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2193022072315216 accum:5.701857089996338\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.22037962079048157 accum:5.729869842529297\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.20619827508926392 accum:5.361155033111572\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.21422867476940155 accum:5.569945335388184\n",
      "----------------------------------------\n",
      "square_root (7806, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2578354477882385 accum:15.727962493896484\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.25401371717453003 accum:15.494836807250977\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.2557259500026703 accum:15.599284172058105\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.25764262676239014 accum:15.716201782226562\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.25841906666755676 accum:15.76356315612793\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2639259696006775 accum:16.099485397338867\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.26565179228782654 accum:16.20475959777832\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.25393426418304443 accum:15.489990234375\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.25652623176574707 accum:15.648101806640625\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.2612670958042145 accum:15.93729305267334\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.259593665599823 accum:15.835213661193848\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.2588869631290436 accum:15.792105674743652\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2659972906112671 accum:16.2258358001709\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.26203209161758423 accum:15.98395824432373\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.25568926334381104 accum:15.5970458984375\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.26309314370155334 accum:16.048683166503906\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.2542180120944977 accum:15.50730037689209\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.2526381015777588 accum:15.41092586517334\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.25273770093917847 accum:15.417000770568848\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.25487470626831055 accum:15.547357559204102\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2648260295391083 accum:16.154388427734375\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.26538488268852234 accum:16.188478469848633\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.2589426338672638 accum:15.795500755310059\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.26992037892341614 accum:16.46514320373535\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.26215559244155884 accum:15.991491317749023\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.2558307647705078 accum:15.605676651000977\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.2546074390411377 accum:15.531055450439453\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.2595187723636627 accum:15.830645561218262\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.26792457699775696 accum:16.343400955200195\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2574838399887085 accum:15.706514358520508\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.26578035950660706 accum:16.212602615356445\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.26194387674331665 accum:15.978577613830566\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2620730996131897 accum:15.98646068572998\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.26229599118232727 accum:16.000057220458984\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.263350248336792 accum:16.06436538696289\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.2624731659889221 accum:16.0108642578125\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.26507568359375 accum:16.16961669921875\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.2537403106689453 accum:15.47815990447998\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.26572296023368835 accum:16.2091007232666\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.26393482089042664 accum:16.100025177001953\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.2595890462398529 accum:15.834933280944824\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.2547801733016968 accum:15.54159164428711\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.2543170154094696 accum:15.513338088989258\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.258822500705719 accum:15.78817367553711\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.26195287704467773 accum:15.979126930236816\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.26089757680892944 accum:15.914753913879395\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.26413944363594055 accum:16.112506866455078\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2637878656387329 accum:16.091060638427734\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.25725609064102173 accum:15.692623138427734\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.26065880060195923 accum:15.900188446044922\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.255464643239975 accum:15.583344459533691\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.25388628244400024 accum:15.48706340789795\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.25798922777175903 accum:15.737343788146973\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.2574204206466675 accum:15.70264720916748\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2627338171005249 accum:16.026763916015625\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.26153871417045593 accum:15.953863143920898\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2619040906429291 accum:15.976149559020996\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.2594638466835022 accum:15.827296257019043\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.25227758288383484 accum:15.388933181762695\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.25970301032066345 accum:15.84188461303711\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.25684255361557007 accum:15.667397499084473\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.25873076915740967 accum:15.782577514648438\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2585308849811554 accum:15.770384788513184\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.26222825050354004 accum:15.99592399597168\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.25641223788261414 accum:15.641146659851074\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2507307827472687 accum:15.29457950592041\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.25726351141929626 accum:15.693075180053711\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.2554878294467926 accum:15.584758758544922\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2576909363269806 accum:15.719147682189941\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.2580581307411194 accum:15.741547584533691\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.25309130549430847 accum:15.438570976257324\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2579175531864166 accum:15.73297119140625\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2567949891090393 accum:15.664495468139648\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.2569349408149719 accum:15.6730318069458\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2509215772151947 accum:15.3062162399292\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.25364792346954346 accum:15.472524642944336\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.2509540617465973 accum:15.308198928833008\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.2587110102176666 accum:15.7813720703125\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.2570156753063202 accum:15.677956581115723\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 79 loss avg:0.26274123787879944 accum:16.0272159576416\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.2564358413219452 accum:15.642587661743164\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.26963287591934204 accum:16.447607040405273\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.2660695016384125 accum:16.230239868164062\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.27227962017059326 accum:16.609058380126953\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.2729398310184479 accum:16.649330139160156\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.2544872760772705 accum:15.523724555969238\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.251035213470459 accum:15.313149452209473\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.2552468478679657 accum:15.57005786895752\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.26351699233055115 accum:16.07453727722168\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2683302164077759 accum:16.368144989013672\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2664397656917572 accum:16.252826690673828\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.267936110496521 accum:16.34410285949707\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.25479960441589355 accum:15.542777061462402\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.26262733340263367 accum:16.020267486572266\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.25802505016326904 accum:15.739529609680176\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.2576482892036438 accum:15.716546058654785\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.2528585195541382 accum:15.424369812011719\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.2521606981754303 accum:15.381803512573242\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.2574492394924164 accum:15.7044038772583\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.2569821774959564 accum:15.675912857055664\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.2592574656009674 accum:15.814706802368164\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.26715195178985596 accum:16.2962703704834\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.2580505609512329 accum:15.741085052490234\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2551247179508209 accum:15.562607765197754\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.2582252621650696 accum:15.751742362976074\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2549668848514557 accum:15.55298137664795\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.2642509341239929 accum:16.119308471679688\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2612083852291107 accum:15.93371295928955\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2614179849624634 accum:15.946497917175293\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.2712828814983368 accum:16.548255920410156\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.26615220308303833 accum:16.23528480529785\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.26577553153038025 accum:16.212308883666992\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.26273250579833984 accum:16.026683807373047\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.25965026021003723 accum:15.838666915893555\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2654310166835785 accum:16.191293716430664\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.2579558789730072 accum:15.735308647155762\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.26084789633750916 accum:15.911722183227539\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2563137412071228 accum:15.635139465332031\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.25880980491638184 accum:15.787398338317871\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.25962767004966736 accum:15.837288856506348\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.2635306417942047 accum:16.07537078857422\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.26871630549430847 accum:16.391695022583008\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.26681849360466003 accum:16.275928497314453\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.27081671357154846 accum:16.519821166992188\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.27749091386795044 accum:16.92694664001465\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.26368841528892517 accum:16.084993362426758\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.26626473665237427 accum:16.242149353027344\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.25546059012413025 accum:15.583097457885742\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.2623600363731384 accum:16.003963470458984\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.26404476165771484 accum:16.106731414794922\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.2632632553577423 accum:16.059059143066406\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.26296913623809814 accum:16.041118621826172\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.2558579742908478 accum:15.60733699798584\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.25613221526145935 accum:15.624065399169922\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2570394277572632 accum:15.679405212402344\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.2557644844055176 accum:15.60163402557373\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.26454290747642517 accum:16.137117385864258\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.27124759554862976 accum:16.546104431152344\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.2567159831523895 accum:15.659675598144531\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.2567528784275055 accum:15.661927223205566\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2695641815662384 accum:16.443416595458984\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.2663469612598419 accum:16.24716567993164\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2639850080013275 accum:16.103086471557617\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.26018401980400085 accum:15.87122631072998\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.2598651349544525 accum:15.851773262023926\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.2647677958011627 accum:16.150836944580078\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.26042476296424866 accum:15.88591194152832\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.2552698850631714 accum:15.571464538574219\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.25888097286224365 accum:15.791740417480469\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2655058801174164 accum:16.195859909057617\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.25323551893234253 accum:15.447367668151855\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.26142096519470215 accum:15.94667911529541\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.26251742243766785 accum:16.01356315612793\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.25068923830986023 accum:15.292044639587402\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.25467464327812195 accum:15.53515338897705\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.2549404799938202 accum:15.551369667053223\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.26297804713249207 accum:16.041662216186523\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.25780296325683594 accum:15.725980758666992\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2663528025150299 accum:16.247522354125977\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2686847150325775 accum:16.389768600463867\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.26323437690734863 accum:16.05729866027832\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.26059263944625854 accum:15.896151542663574\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.26577213406562805 accum:16.212100982666016\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2595803141593933 accum:15.834399223327637\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.2588946521282196 accum:15.792573928833008\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2572716474533081 accum:15.693572044372559\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2657410800457001 accum:16.210206985473633\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.2697141468524933 accum:16.452564239501953\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2648366391658783 accum:16.1550350189209\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.25999927520751953 accum:15.859955787658691\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.2589224576950073 accum:15.794271469116211\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.25903627276420593 accum:15.801213264465332\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.24906125664710999 accum:15.192737579345703\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.2569660544395447 accum:15.674930572509766\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.26011085510253906 accum:15.8667631149292\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.2621517777442932 accum:15.99125862121582\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.2616466283798218 accum:15.960445404052734\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.2662237584590912 accum:16.23965072631836\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.26001808047294617 accum:15.861103057861328\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.26002517342567444 accum:15.861536979675293\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.25208815932273865 accum:15.377379417419434\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.2556975185871124 accum:15.597550392150879\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.2605133652687073 accum:15.891316413879395\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.2526121437549591 accum:15.409341812133789\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.26388540863990784 accum:16.09701156616211\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.25683173537254333 accum:15.66673755645752\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.2546302378177643 accum:15.532444953918457\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.25616124272346497 accum:15.625836372375488\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2554182708263397 accum:15.58051586151123\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2494729459285736 accum:15.217850685119629\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.2636067867279053 accum:16.080015182495117\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2514761984348297 accum:15.340048789978027\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.25199073553085327 accum:15.37143611907959\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.2492789477109909 accum:15.206016540527344\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.24828073382377625 accum:15.145125389099121\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.24857211112976074 accum:15.1628999710083\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.25397953391075134 accum:15.492753028869629\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2590864598751068 accum:15.804274559020996\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.26061731576919556 accum:15.89765739440918\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.2517595887184143 accum:15.357336044311523\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.25262635946273804 accum:15.410209655761719\n",
      "----------------------------------------\n",
      "square (7686, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2974766790866852 accum:18.14607810974121\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.29279741644859314 accum:17.86064338684082\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.29104113578796387 accum:17.753509521484375\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.30690667033195496 accum:18.7213077545166\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.30146336555480957 accum:18.389266967773438\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.3012568950653076 accum:18.376670837402344\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.29273512959480286 accum:17.856843948364258\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.3002188503742218 accum:18.313350677490234\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.3029322028160095 accum:18.478864669799805\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.2962316572666168 accum:18.070131301879883\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.29530441761016846 accum:18.01357078552246\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.29151415824890137 accum:17.782363891601562\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2989290952682495 accum:18.234676361083984\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.294594943523407 accum:17.970293045043945\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.3091939389705658 accum:18.86083221435547\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.3034767508506775 accum:18.512083053588867\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.29651957750320435 accum:18.087696075439453\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.29532384872436523 accum:18.014755249023438\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.30324456095695496 accum:18.4979190826416\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.3174280524253845 accum:19.36311149597168\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.30736109614372253 accum:18.749027252197266\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.29658108949661255 accum:18.091447830200195\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.2932504415512085 accum:17.888277053833008\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.29140505194664 accum:17.77570915222168\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.2885955572128296 accum:17.60433006286621\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.2946179509162903 accum:17.971696853637695\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.29598551988601685 accum:18.055118560791016\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.30741986632347107 accum:18.752613067626953\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.30276650190353394 accum:18.46875762939453\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2930227518081665 accum:17.8743896484375\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.29095837473869324 accum:17.748462677001953\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.31153807044029236 accum:19.00382423400879\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32 loss avg:0.3010217547416687 accum:18.362327575683594\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.30261239409446716 accum:18.4593563079834\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.2907116115093231 accum:17.733409881591797\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.28875720500946045 accum:17.61419105529785\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.29322972893714905 accum:17.887014389038086\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.2968899607658386 accum:18.110288619995117\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.3038932681083679 accum:18.537490844726562\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.2880212366580963 accum:17.56929588317871\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.29122287034988403 accum:17.764596939086914\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.286226749420166 accum:17.4598331451416\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.29436832666397095 accum:17.95646858215332\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.3062964975833893 accum:18.6840877532959\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2866726517677307 accum:17.487031936645508\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.28390583395957947 accum:17.318256378173828\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.28683844208717346 accum:17.497146606445312\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2859707176685333 accum:17.4442138671875\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.2988632619380951 accum:18.23065948486328\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.3054128885269165 accum:18.63018798828125\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.29421329498291016 accum:17.947011947631836\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.292251318693161 accum:17.82733154296875\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.2868868410587311 accum:17.500099182128906\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.28641602396965027 accum:17.471378326416016\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.28692513704299927 accum:17.50243377685547\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.29947829246520996 accum:18.268177032470703\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2985612154006958 accum:18.212234497070312\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.29037004709243774 accum:17.712574005126953\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.29926156997680664 accum:18.25495719909668\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.289350301027298 accum:17.65036964416504\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.2924574017524719 accum:17.839902877807617\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.30511710047721863 accum:18.612144470214844\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.29486069083213806 accum:17.98650360107422\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.29079264402389526 accum:17.738351821899414\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.2856041491031647 accum:17.42185401916504\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2889545261859894 accum:17.6262264251709\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.29080453515052795 accum:17.739078521728516\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.29052186012268066 accum:17.721834182739258\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2772935628890991 accum:16.914907455444336\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.28704574704170227 accum:17.50979232788086\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.29066330194473267 accum:17.7304630279541\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2886960804462433 accum:17.610462188720703\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2872494161128998 accum:17.522214889526367\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.29179903864860535 accum:17.799741744995117\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2816838324069977 accum:17.182714462280273\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.2868330776691437 accum:17.49681854248047\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.28130772709846497 accum:17.159772872924805\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.2813534140586853 accum:17.162559509277344\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.29068523645401 accum:17.731800079345703\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.2921846807003021 accum:17.823266983032227\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.2905987501144409 accum:17.726524353027344\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.294334352016449 accum:17.954397201538086\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.28502342104911804 accum:17.386428833007812\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.28934717178344727 accum:17.650178909301758\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.2864459156990051 accum:17.473201751708984\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.29414206743240356 accum:17.94266700744629\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.29273268580436707 accum:17.8566951751709\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.30132052302360535 accum:18.380552291870117\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.2995952069759369 accum:18.27530860900879\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.28882893919944763 accum:17.618566513061523\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2945614457130432 accum:17.96824836730957\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.2862084209918976 accum:17.458715438842773\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.2827325463294983 accum:17.246686935424805\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.2923240065574646 accum:17.83176612854004\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.28431493043899536 accum:17.343212127685547\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.2853225767612457 accum:17.404678344726562\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.29462045431137085 accum:17.97184944152832\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.2882632911205292 accum:17.584062576293945\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.28425467014312744 accum:17.339536666870117\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.28927746415138245 accum:17.645925521850586\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.2770113945007324 accum:16.897695541381836\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.2835409641265869 accum:17.29599952697754\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.292729914188385 accum:17.856525421142578\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2969880998134613 accum:18.116275787353516\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.2905433475971222 accum:17.72314453125\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2914271950721741 accum:17.77705955505371\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.29361966252326965 accum:17.91080093383789\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.3055921792984009 accum:18.641124725341797\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2898499071598053 accum:17.680845260620117\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.28932005167007446 accum:17.648523330688477\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2828030288219452 accum:17.250986099243164\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.2894303500652313 accum:17.65525245666504\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.2873719036579132 accum:17.529687881469727\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.281867653131485 accum:17.193927764892578\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.285846471786499 accum:17.436635971069336\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.28792473673820496 accum:17.56340980529785\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.281212717294693 accum:17.153976440429688\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2889680862426758 accum:17.62705421447754\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.2855576276779175 accum:17.419015884399414\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.29292359948158264 accum:17.86834144592285\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.29688552021980286 accum:18.110017776489258\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.2950862944126129 accum:18.00026512145996\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.3013553321361542 accum:18.38267707824707\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.2950863540172577 accum:18.000268936157227\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.29148972034454346 accum:17.780874252319336\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.291037917137146 accum:17.753313064575195\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.2868400812149048 accum:17.49724578857422\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2818998396396637 accum:17.195890426635742\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.28724151849746704 accum:17.5217342376709\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.2827390432357788 accum:17.247081756591797\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.2807846665382385 accum:17.127864837646484\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.28454723954200745 accum:17.35738182067871\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.27647483348846436 accum:16.864965438842773\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.288136750459671 accum:17.576343536376953\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.29133498668670654 accum:17.771434783935547\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.2931080758571625 accum:17.879592895507812\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.28478169441223145 accum:17.371685028076172\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.2943251430988312 accum:17.953834533691406\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.27584654092788696 accum:16.82663917541504\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.28600504994392395 accum:17.446308135986328\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2851814329624176 accum:17.396068572998047\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.28548935055732727 accum:17.414852142333984\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2955537736415863 accum:18.02878189086914\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2861591875553131 accum:17.455711364746094\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.2827829122543335 accum:17.249757766723633\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.28020980954170227 accum:17.09280014038086\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.2783629894256592 accum:16.98014259338379\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.27843159437179565 accum:16.98432731628418\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.2803126275539398 accum:17.099071502685547\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.28321605920791626 accum:17.276180267333984\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.2792147099971771 accum:17.0320987701416\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.3081827163696289 accum:18.79914665222168\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.2906606197357178 accum:17.73029899597168\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.28563225269317627 accum:17.423568725585938\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.29422271251678467 accum:17.947586059570312\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.3045390546321869 accum:18.57688331604004\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.2974533438682556 accum:18.144655227661133\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.28943243622779846 accum:17.655380249023438\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2874489724636078 accum:17.534387588500977\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2879788875579834 accum:17.566713333129883\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.28275173902511597 accum:17.24785614013672\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.2788756489753723 accum:17.011415481567383\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.28941452503204346 accum:17.654287338256836\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2938174605369568 accum:17.922866821289062\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.28042134642601013 accum:17.105703353881836\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2813214063644409 accum:17.160606384277344\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.28307560086250305 accum:17.26761245727539\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.27925360202789307 accum:17.03447151184082\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2858841121196747 accum:17.438932418823242\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.2838687300682068 accum:17.315994262695312\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.28302890062332153 accum:17.2647647857666\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.28529641032218933 accum:17.4030818939209\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.2789815068244934 accum:17.017873764038086\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.27677834033966064 accum:16.883480072021484\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2784290909767151 accum:16.984174728393555\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.2813442647457123 accum:17.16200065612793\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.27731916308403015 accum:16.91646957397461\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.302179217338562 accum:18.432933807373047\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.30433234572410583 accum:18.564273834228516\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.29951539635658264 accum:18.27044105529785\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.2989978492259979 accum:18.23887062072754\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.29577890038490295 accum:18.04251480102539\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.29039299488067627 accum:17.713973999023438\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.2849437892436981 accum:17.381572723388672\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.286391943693161 accum:17.46990966796875\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.2844075858592987 accum:17.34886360168457\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.2887774705886841 accum:17.615427017211914\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.29252713918685913 accum:17.84415626525879\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.30989277362823486 accum:18.903459548950195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2984920144081116 accum:18.2080135345459\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.2849973440170288 accum:17.384838104248047\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.28872036933898926 accum:17.6119441986084\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.30554094910621643 accum:18.637998580932617\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.28927260637283325 accum:17.6456298828125\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2990575432777405 accum:18.242511749267578\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.3002774715423584 accum:18.316926956176758\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.29490119218826294 accum:17.98897361755371\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2922423481941223 accum:17.826784133911133\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.2911887466907501 accum:17.762514114379883\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.2958543598651886 accum:18.047117233276367\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.28958821296691895 accum:17.66488265991211\n",
      "----------------------------------------\n",
      "frequency (7920, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2740820348262787 accum:16.993085861206055\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.268917977809906 accum:16.672914505004883\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.27287134528160095 accum:16.91802406311035\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.2686271667480469 accum:16.654884338378906\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.2651056945323944 accum:16.436553955078125\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2664746344089508 accum:16.521427154541016\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.27536576986312866 accum:17.072677612304688\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.2715025246143341 accum:16.83315658569336\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.26850441098213196 accum:16.647274017333984\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.27214473485946655 accum:16.872974395751953\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.26906299591064453 accum:16.68190574645996\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.2688452899456024 accum:16.66840934753418\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.27118661999702454 accum:16.813570022583008\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.26695916056632996 accum:16.551467895507812\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.2610955834388733 accum:16.18792724609375\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.26323288679122925 accum:16.3204402923584\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.2625303566455841 accum:16.27688217163086\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.26601874828338623 accum:16.493162155151367\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.25642749667167664 accum:15.898505210876465\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.2652793526649475 accum:16.44732093811035\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2736090123653412 accum:16.96375846862793\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.27167412638664246 accum:16.843795776367188\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.26198071241378784 accum:16.24280548095703\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.26001232862472534 accum:16.120765686035156\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.26919689774513245 accum:16.690208435058594\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.2705053985118866 accum:16.77133560180664\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.26947087049484253 accum:16.707195281982422\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.2684212327003479 accum:16.64211654663086\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2658146321773529 accum:16.480506896972656\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2612123191356659 accum:16.19516372680664\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.26254284381866455 accum:16.27765655517578\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.2666133940219879 accum:16.530031204223633\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2605854570865631 accum:16.156299591064453\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.2588205337524414 accum:16.046873092651367\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.259679913520813 accum:16.100154876708984\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.25873038172721863 accum:16.041284561157227\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.25310811400413513 accum:15.692703247070312\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.26253166794776917 accum:16.27696418762207\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.2634443938732147 accum:16.333553314208984\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.27009767293930054 accum:16.746055603027344\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.27017924189567566 accum:16.751113891601562\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.26123419404029846 accum:16.19651985168457\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.2702403664588928 accum:16.75490379333496\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.2655293047428131 accum:16.462818145751953\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2649121880531311 accum:16.424556732177734\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.26815420389175415 accum:16.625560760498047\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.2730275094509125 accum:16.927705764770508\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2706179618835449 accum:16.7783145904541\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.26988300681114197 accum:16.732746124267578\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.26820608973503113 accum:16.6287784576416\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.26521018147468567 accum:16.443031311035156\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.26860660314559937 accum:16.653610229492188\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.26779770851135254 accum:16.603458404541016\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.26375728845596313 accum:16.35295295715332\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2698976397514343 accum:16.733654022216797\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.2631874680519104 accum:16.317623138427734\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.25751322507858276 accum:15.9658203125\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.26427072286605835 accum:16.384784698486328\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.26199963688850403 accum:16.24397850036621\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.25944411754608154 accum:16.085535049438477\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.2625301778316498 accum:16.276870727539062\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.2722378075122833 accum:16.87874412536621\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2622276246547699 accum:16.258113861083984\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2680377960205078 accum:16.618343353271484\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.266225665807724 accum:16.505990982055664\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.26252609491348267 accum:16.27661895751953\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.2634845972061157 accum:16.33604621887207\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.25860050320625305 accum:16.033231735229492\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2611430585384369 accum:16.19087028503418\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.26745086908340454 accum:16.581954956054688\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.264537513256073 accum:16.40132713317871\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.26335933804512024 accum:16.328279495239258\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.26321524381637573 accum:16.319345474243164\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.255855530500412 accum:15.863043785095215\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.26228731870651245 accum:16.26181411743164\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.25587716698646545 accum:15.864384651184082\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.2582583725452423 accum:16.012020111083984\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.26597651839256287 accum:16.49054527282715\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.2666373550891876 accum:16.531517028808594\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.25468868017196655 accum:15.790699005126953\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.260488361120224 accum:16.150278091430664\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.26213645935058594 accum:16.252460479736328\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.25689539313316345 accum:15.927515029907227\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.2540476322174072 accum:15.750953674316406\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.2598532438278198 accum:16.11090087890625\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.25518494844436646 accum:15.821466445922852\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.26079291105270386 accum:16.169160842895508\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.2713256776332855 accum:16.822193145751953\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.2681669294834137 accum:16.62635040283203\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.27100110054016113 accum:16.80206871032715\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2619677484035492 accum:16.242000579833984\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.2650126814842224 accum:16.4307861328125\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.25597837567329407 accum:15.870659828186035\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.26451826095581055 accum:16.40013313293457\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.26515838503837585 accum:16.439821243286133\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.26499050855636597 accum:16.429412841796875\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.26772624254226685 accum:16.599027633666992\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.262540340423584 accum:16.277502059936523\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.26131895184516907 accum:16.20177459716797\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.2706248164176941 accum:16.77873992919922\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.271542489528656 accum:16.835634231567383\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.26864680647850037 accum:16.656103134155273\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.2565460205078125 accum:15.905853271484375\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.25820043683052063 accum:16.008426666259766\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.2623448073863983 accum:16.265378952026367\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2655075788497925 accum:16.461469650268555\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.26769396662712097 accum:16.597026824951172\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.26100045442581177 accum:16.18202781677246\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.26268836855888367 accum:16.286680221557617\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.27080076932907104 accum:16.789648056030273\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.26815265417099 accum:16.625465393066406\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.265530526638031 accum:16.462892532348633\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.2616502046585083 accum:16.222312927246094\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.2697635889053345 accum:16.725343704223633\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2634895145893097 accum:16.336349487304688\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.25668686628341675 accum:15.914586067199707\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.2692497670650482 accum:16.693485260009766\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.26076653599739075 accum:16.167526245117188\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.25977620482444763 accum:16.106124877929688\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.2566901743412018 accum:15.914791107177734\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.25782015919685364 accum:15.98484992980957\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.2617674469947815 accum:16.229581832885742\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.25796645879745483 accum:15.993921279907227\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.2583447992801666 accum:16.017377853393555\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.26074498891830444 accum:16.166189193725586\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.25896191596984863 accum:16.055639266967773\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.2560562193393707 accum:15.87548542022705\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2575785517692566 accum:15.969870567321777\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.2644158601760864 accum:16.393783569335938\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.25867313146591187 accum:16.037734985351562\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.2574760913848877 accum:15.963518142700195\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.26105624437332153 accum:16.185487747192383\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.26232871413230896 accum:16.264381408691406\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.26715248823165894 accum:16.56345558166504\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.26628005504608154 accum:16.509363174438477\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.26365169882774353 accum:16.346405029296875\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.2644946873188019 accum:16.398670196533203\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.2829035520553589 accum:17.540019989013672\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.26151755452156067 accum:16.214088439941406\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.2613336145877838 accum:16.20268440246582\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2624858319759369 accum:16.27412223815918\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.2583874464035034 accum:16.020021438598633\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 142 loss avg:0.26069751381874084 accum:16.163246154785156\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.26182034611701965 accum:16.23286247253418\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.255527138710022 accum:15.842683792114258\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.271962434053421 accum:16.861671447753906\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.26904943585395813 accum:16.68106460571289\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.2634642720222473 accum:16.33478546142578\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.25808772444725037 accum:16.001440048217773\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2688906490802765 accum:16.671220779418945\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.26885804533958435 accum:16.669198989868164\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.26258984208106995 accum:16.28057098388672\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.25505954027175903 accum:15.813691139221191\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.259986013174057 accum:16.11913299560547\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.26533836126327515 accum:16.450979232788086\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.2676490843296051 accum:16.5942440032959\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.2661135792732239 accum:16.499042510986328\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.2551078498363495 accum:15.816686630249023\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.25373637676239014 accum:15.731656074523926\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2584075331687927 accum:16.02126693725586\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.25194793939590454 accum:15.620772361755371\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.25271278619766235 accum:15.668193817138672\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.25704193115234375 accum:15.936599731445312\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2624848783016205 accum:16.274063110351562\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.2618647813796997 accum:16.23561668395996\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2611948847770691 accum:16.19408416748047\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2636658549308777 accum:16.3472843170166\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.26739978790283203 accum:16.578786849975586\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2625284790992737 accum:16.276765823364258\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.2638727128505707 accum:16.360109329223633\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.2618153989315033 accum:16.232555389404297\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.2628907859325409 accum:16.29922866821289\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.26296448707580566 accum:16.30379867553711\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.2574619650840759 accum:15.962641716003418\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.25942230224609375 accum:16.084182739257812\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.26072970032691956 accum:16.165241241455078\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.26647281646728516 accum:16.52131462097168\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.26248878240585327 accum:16.27430534362793\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.26828819513320923 accum:16.633869171142578\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.27366164326667786 accum:16.967021942138672\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.262285977602005 accum:16.261730194091797\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.26215070486068726 accum:16.25334358215332\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.25940707325935364 accum:16.08323860168457\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.2580341696739197 accum:15.99811840057373\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.2595900297164917 accum:16.094581604003906\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.25900864601135254 accum:16.058536529541016\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.2641209661960602 accum:16.375499725341797\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.2552207112312317 accum:15.823684692382812\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2624424397945404 accum:16.27143096923828\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2534291446208954 accum:15.712608337402344\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.26008060574531555 accum:16.124998092651367\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2598012089729309 accum:16.107675552368164\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.24995678663253784 accum:15.497321128845215\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.2581121325492859 accum:16.002952575683594\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2563821077346802 accum:15.895691871643066\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.2611459791660309 accum:16.191051483154297\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.2602073848247528 accum:16.132858276367188\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2612733542919159 accum:16.19894790649414\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.271457314491272 accum:16.830354690551758\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.2560836672782898 accum:15.877188682556152\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.2614081799983978 accum:16.207307815551758\n",
      "----------------------------------------\n",
      "round (7690, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.29270139336586 accum:17.854785919189453\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.2941388487815857 accum:17.94247055053711\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.2816929519176483 accum:17.183271408081055\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.2800508439540863 accum:17.08310317993164\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.28286826610565186 accum:17.25496482849121\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.27822989225387573 accum:16.97202491760254\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.2810009717941284 accum:17.14105987548828\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.2765713632106781 accum:16.870853424072266\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.2849782705307007 accum:17.38367462158203\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.28507763147354126 accum:17.38973617553711\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.28127291798591614 accum:17.15764808654785\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.29687070846557617 accum:18.109113693237305\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2910749316215515 accum:17.755571365356445\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.2920067012310028 accum:17.812410354614258\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.27588802576065063 accum:16.82917022705078\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.2852773368358612 accum:17.401918411254883\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.28015297651290894 accum:17.089332580566406\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.2884095311164856 accum:17.59298324584961\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.285475492477417 accum:17.414005279541016\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.2918742299079895 accum:17.80432891845703\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2849535048007965 accum:17.382164001464844\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.2783534526824951 accum:16.97956085205078\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.28228461742401123 accum:17.219362258911133\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.27907922863960266 accum:17.023834228515625\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.28183794021606445 accum:17.192115783691406\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.2816683053970337 accum:17.1817684173584\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.28420886397361755 accum:17.336742401123047\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.282968133687973 accum:17.261056900024414\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2941519320011139 accum:17.943269729614258\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2975752055644989 accum:18.152088165283203\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.2778056263923645 accum:16.946144104003906\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.2896061837673187 accum:17.665977478027344\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.29348519444465637 accum:17.902597427368164\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.28678274154663086 accum:17.49374771118164\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.2795935571193695 accum:17.055208206176758\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.2764934003353119 accum:16.866098403930664\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.28284794092178345 accum:17.253725051879883\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.2769084572792053 accum:16.891416549682617\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.28741779923439026 accum:17.532485961914062\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.2869332432746887 accum:17.5029296875\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.2829973101615906 accum:17.262836456298828\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.28080040216445923 accum:17.128826141357422\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.289840430021286 accum:17.680267333984375\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.2869446277618408 accum:17.503623962402344\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.28257039189338684 accum:17.23679542541504\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.2801743745803833 accum:17.09063720703125\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.2757566273212433 accum:16.821155548095703\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.28424569964408875 accum:17.3389892578125\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.2832724452018738 accum:17.279619216918945\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.28447702527046204 accum:17.353099822998047\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.28203460574150085 accum:17.204111099243164\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.2852545380592346 accum:17.400527954101562\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.2974748909473419 accum:18.14596939086914\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.28092843294143677 accum:17.136634826660156\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.27746060490608215 accum:16.925098419189453\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.29859885573387146 accum:18.21453094482422\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2904655933380127 accum:17.718402862548828\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.28353795409202576 accum:17.29581642150879\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.285659521818161 accum:17.42523193359375\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.28725382685661316 accum:17.522483825683594\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.29000455141067505 accum:17.690279006958008\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.29144829511642456 accum:17.77834701538086\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2869904637336731 accum:17.506420135498047\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.28615567088127136 accum:17.45549774169922\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.2858958840370178 accum:17.43964958190918\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.28472694754600525 accum:17.368345260620117\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.2884042263031006 accum:17.59265899658203\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.29021474719047546 accum:17.703100204467773\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2826633155345917 accum:17.242464065551758\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.27610036730766296 accum:16.84212303161621\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.2755526304244995 accum:16.808712005615234\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2800467908382416 accum:17.082855224609375\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2784338891506195 accum:16.984468460083008\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.2720624506473541 accum:16.595809936523438\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.27953454852104187 accum:17.05160903930664\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.27400389313697815 accum:16.7142391204834\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.27951085567474365 accum:17.05016326904297\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.2778366208076477 accum:16.948034286499023\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.26825177669525146 accum:16.363359451293945\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.26508858799934387 accum:16.1704044342041\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.2795303761959076 accum:17.051353454589844\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.28786006569862366 accum:17.559465408325195\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.29137325286865234 accum:17.77376937866211\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.27939650416374207 accum:17.043188095092773\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.28153836727142334 accum:17.17384147644043\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.282480925321579 accum:17.23133659362793\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.27664652466773987 accum:16.875438690185547\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.2754172682762146 accum:16.80045509338379\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.27446869015693665 accum:16.742591857910156\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.27592453360557556 accum:16.831398010253906\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2775256633758545 accum:16.929065704345703\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.267220675945282 accum:16.30046272277832\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.27239134907722473 accum:16.615873336791992\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.2753385603427887 accum:16.795652389526367\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.2825765907764435 accum:17.237173080444336\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.28069189190864563 accum:17.12220573425293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.276285856962204 accum:16.853437423706055\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.2811005413532257 accum:17.14713478088379\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.2777811884880066 accum:16.944652557373047\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.2836730480194092 accum:17.30405616760254\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.27876269817352295 accum:17.004526138305664\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.2727026343345642 accum:16.63486099243164\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.2749827206134796 accum:16.77394676208496\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2776035666465759 accum:16.933818817138672\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.27810361981391907 accum:16.96432113647461\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2744910418987274 accum:16.743953704833984\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.2674804627895355 accum:16.316308975219727\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2869422435760498 accum:17.503477096557617\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2781201899051666 accum:16.96533203125\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.2770167887210846 accum:16.898025512695312\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2843307852745056 accum:17.344179153442383\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.2777526378631592 accum:16.94291114807129\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.2749943137168884 accum:16.774654388427734\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.27496567368507385 accum:16.772907257080078\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2845055162906647 accum:17.35483741760254\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.2796357274055481 accum:17.057781219482422\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.26763418316841125 accum:16.325685501098633\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2761920988559723 accum:16.847719192504883\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.280463844537735 accum:17.108295440673828\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.292217880487442 accum:17.825292587280273\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.2886676490306854 accum:17.608728408813477\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.2917417585849762 accum:17.796247482299805\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2874305844306946 accum:17.533266067504883\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.2780044674873352 accum:16.95827293395996\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.29178285598754883 accum:17.798755645751953\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.2952039837837219 accum:18.007444381713867\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.2831737995147705 accum:17.273603439331055\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2816212475299835 accum:17.178897857666016\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.28521567583084106 accum:17.398157119750977\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.27222940325737 accum:16.605995178222656\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.29619577527046204 accum:18.067943572998047\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.2900177836418152 accum:17.691085815429688\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.2854667603969574 accum:17.41347312927246\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.2858797609806061 accum:17.43866729736328\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2701888084411621 accum:16.481517791748047\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.274260938167572 accum:16.729917526245117\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.27175360918045044 accum:16.57697105407715\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.271306574344635 accum:16.549701690673828\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.29064008593559265 accum:17.729045867919922\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.2896828055381775 accum:17.670652389526367\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.28813594579696655 accum:17.5762939453125\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.2701184153556824 accum:16.477224349975586\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.26910480856895447 accum:16.415393829345703\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2718965411186218 accum:16.585689544677734\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.26866069436073303 accum:16.388303756713867\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.27590227127075195 accum:16.830039978027344\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.27087217569351196 accum:16.523202896118164\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.26667648553848267 accum:16.26726722717285\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.2745973467826843 accum:16.750438690185547\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2781493365764618 accum:16.96710968017578\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.283458948135376 accum:17.290996551513672\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.28306493163108826 accum:17.2669620513916\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.2816097140312195 accum:17.178194046020508\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.2753295302391052 accum:16.795103073120117\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.27216970920562744 accum:16.602354049682617\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.273446649312973 accum:16.680246353149414\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.289931982755661 accum:17.68585205078125\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.2811717689037323 accum:17.151479721069336\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2848564684391022 accum:17.376245498657227\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.27373558282852173 accum:16.697872161865234\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.28032657504081726 accum:17.09992218017578\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.28911131620407104 accum:17.635791778564453\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.2706505358219147 accum:16.50968360900879\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2729916274547577 accum:16.652490615844727\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.27641668915748596 accum:16.861419677734375\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2823985815048218 accum:17.226314544677734\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2769981920719147 accum:16.89689064025879\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.28450411558151245 accum:17.354751586914062\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2932700216770172 accum:17.88947296142578\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.2796400487422943 accum:17.05804443359375\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.267306387424469 accum:16.30569076538086\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.2953118681907654 accum:18.01402473449707\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.2939484119415283 accum:17.93085479736328\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.28448057174682617 accum:17.353315353393555\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2816625237464905 accum:17.181415557861328\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.27901387214660645 accum:17.019847869873047\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.2669003903865814 accum:16.28092384338379\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.2782433032989502 accum:16.972843170166016\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.27643290162086487 accum:16.862407684326172\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2768486440181732 accum:16.887767791748047\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.2745663523674011 accum:16.74854850769043\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.28898996114730835 accum:17.628389358520508\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.28618741035461426 accum:17.457433700561523\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.2873746454715729 accum:17.52985382080078\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.27699577808380127 accum:16.896743774414062\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.276137113571167 accum:16.844364166259766\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.2696825861930847 accum:16.450637817382812\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.27066347002983093 accum:16.510473251342773\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2707604169845581 accum:16.516386032104492\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.274583101272583 accum:16.749570846557617\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.27177736163139343 accum:16.578420639038086\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2682992219924927 accum:16.366252899169922\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2634129524230957 accum:16.068191528320312\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.27221471071243286 accum:16.605098724365234\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2658312916755676 accum:16.215709686279297\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.26322466135025024 accum:16.056705474853516\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.2688013017177582 accum:16.396881103515625\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2756193280220032 accum:16.812780380249023\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.278103768825531 accum:16.964330673217773\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.27067673206329346 accum:16.511281967163086\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.26532483100891113 accum:16.184816360473633\n",
      "----------------------------------------\n",
      "tanh (7586, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2750170826911926 accum:16.50102424621582\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.2638128995895386 accum:15.82877254486084\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.2670702040195465 accum:16.024211883544922\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.26717355847358704 accum:16.030412673950195\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.2735992670059204 accum:16.41595458984375\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.26729169487953186 accum:16.037500381469727\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.27671217918395996 accum:16.60272979736328\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.2766328752040863 accum:16.597970962524414\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.2771640419960022 accum:16.629840850830078\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.2719603180885315 accum:16.317617416381836\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.27231866121292114 accum:16.33911895751953\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.2756439745426178 accum:16.538637161254883\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.27744489908218384 accum:16.646692276000977\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.2838437259197235 accum:17.030622482299805\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.2764516770839691 accum:16.587099075317383\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.2677994668483734 accum:16.06796646118164\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.26743072271347046 accum:16.04584312438965\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.2696429193019867 accum:16.178573608398438\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.2723653018474579 accum:16.341917037963867\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.26962870359420776 accum:16.17772102355957\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2795926332473755 accum:16.775556564331055\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.27248406410217285 accum:16.349042892456055\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.27290329337120056 accum:16.374197006225586\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.2751980423927307 accum:16.51188087463379\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.2706030309200287 accum:16.236181259155273\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.26667726039886475 accum:16.000635147094727\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.26776131987571716 accum:16.065677642822266\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.27143070101737976 accum:16.28584098815918\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2717430889606476 accum:16.304584503173828\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2730376720428467 accum:16.382259368896484\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.2684071660041809 accum:16.104429244995117\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.28049352765083313 accum:16.82961082458496\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2645423412322998 accum:15.872539520263672\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.26600417494773865 accum:15.960249900817871\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.2726508677005768 accum:16.359050750732422\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.2614693343639374 accum:15.688159942626953\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.2742079794406891 accum:16.452478408813477\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.2726476788520813 accum:16.35886001586914\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.27197396755218506 accum:16.318437576293945\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.2733685374259949 accum:16.402111053466797\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.2701006531715393 accum:16.206037521362305\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.2808665335178375 accum:16.851991653442383\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.27475112676620483 accum:16.48506736755371\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.2759920358657837 accum:16.559520721435547\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2602112889289856 accum:15.612676620483398\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.2750772535800934 accum:16.504634857177734\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.2753424048423767 accum:16.520544052124023\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2761039733886719 accum:16.566238403320312\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.2736072242259979 accum:16.416433334350586\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 49 loss avg:0.27995583415031433 accum:16.79734992980957\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.267860472202301 accum:16.071626663208008\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.26982778310775757 accum:16.189666748046875\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.2599453926086426 accum:15.596723556518555\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.26654714345932007 accum:15.992827415466309\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2696613371372223 accum:16.17967987060547\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.2737243175506592 accum:16.423458099365234\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2741749882698059 accum:16.450498580932617\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.2756825387477875 accum:16.540950775146484\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.28050410747528076 accum:16.830245971679688\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.270999014377594 accum:16.259939193725586\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.27316123247146606 accum:16.389673233032227\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.2627538740634918 accum:15.765231132507324\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.26326197385787964 accum:15.795717239379883\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2654706537723541 accum:15.928237915039062\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.2690609097480774 accum:16.143653869628906\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2704527974128723 accum:16.2271671295166\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.27610790729522705 accum:16.56647300720215\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.2636958956718445 accum:15.821752548217773\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.26445281505584717 accum:15.867167472839355\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.25711700320243835 accum:15.427019119262695\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.2707013487815857 accum:16.242080688476562\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2621462941169739 accum:15.728776931762695\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2701924741268158 accum:16.2115478515625\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.271784245967865 accum:16.30705451965332\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2788815498352051 accum:16.732892990112305\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.2801637351512909 accum:16.809823989868164\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.27518096566200256 accum:16.51085662841797\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.27688953280448914 accum:16.613370895385742\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.26833242177963257 accum:16.099945068359375\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.26931920647621155 accum:16.159151077270508\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.2617551386356354 accum:15.705307960510254\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.26488587260246277 accum:15.89315128326416\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.26950663328170776 accum:16.17039680480957\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.2694237530231476 accum:16.165424346923828\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.2688663601875305 accum:16.131980895996094\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.26169729232788086 accum:15.701836585998535\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.26447150111198425 accum:15.86828899383545\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.26431113481521606 accum:15.85866641998291\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.2725369334220886 accum:16.352214813232422\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2683660686016083 accum:16.10196304321289\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2685704827308655 accum:16.114227294921875\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.26070743799209595 accum:15.642444610595703\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.2658735513687134 accum:15.952411651611328\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.2685035765171051 accum:16.110214233398438\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.26873230934143066 accum:16.123937606811523\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.2597719430923462 accum:15.586316108703613\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.26878809928894043 accum:16.12728500366211\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.26229193806648254 accum:15.737515449523926\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.2597793936729431 accum:15.586763381958008\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.26635506749153137 accum:15.981303215026855\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.26655855774879456 accum:15.993513107299805\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.26523542404174805 accum:15.914125442504883\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.2605862617492676 accum:15.635175704956055\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2647757828235626 accum:15.88654613494873\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.268383264541626 accum:16.102994918823242\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.264281302690506 accum:15.856877326965332\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.2673046588897705 accum:16.038278579711914\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2644890546798706 accum:15.869342803955078\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2670220136642456 accum:16.021320343017578\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.27514201402664185 accum:16.508520126342773\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2690371870994568 accum:16.142230987548828\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.26307040452957153 accum:15.784223556518555\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.26315730810165405 accum:15.789438247680664\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.26759612560272217 accum:16.055767059326172\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2756090760231018 accum:16.536542892456055\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.27684926986694336 accum:16.6109561920166\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.27340370416641235 accum:16.404220581054688\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2713741660118103 accum:16.28244972229004\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.272905170917511 accum:16.374309539794922\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.2581501305103302 accum:15.489006996154785\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.2648341953754425 accum:15.890050888061523\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.2566205561161041 accum:15.397233009338379\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2711998522281647 accum:16.271989822387695\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.26428812742233276 accum:15.85728645324707\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.2656015455722809 accum:15.936092376708984\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.2705411911010742 accum:16.232471466064453\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.27061527967453003 accum:16.236915588378906\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2717702090740204 accum:16.306211471557617\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.26735568046569824 accum:16.041339874267578\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.25790464878082275 accum:15.474278450012207\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.25944676995277405 accum:15.566805839538574\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.263369083404541 accum:15.802145004272461\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.26961570978164673 accum:16.17694091796875\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.27078455686569214 accum:16.247072219848633\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2670004665851593 accum:16.02002716064453\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.2751992642879486 accum:16.51195526123047\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.2807425558567047 accum:16.844552993774414\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.26361218094825745 accum:15.816730499267578\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.2692664861679077 accum:16.155988693237305\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.26752299070358276 accum:16.05137825012207\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.26249727606773376 accum:15.749835014343262\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.2649640738964081 accum:15.897843360900879\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2583632171154022 accum:15.501791954040527\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2618240714073181 accum:15.709443092346191\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.2626323103904724 accum:15.757938385009766\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.26241323351860046 accum:15.744792938232422\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.26907166838645935 accum:16.144298553466797\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.26308807730674744 accum:15.785283088684082\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.2605170011520386 accum:15.631019592285156\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2651417553424835 accum:15.908504486083984\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.25993600487709045 accum:15.596158981323242\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.26295116543769836 accum:15.777069091796875\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.2607969045639038 accum:15.64781379699707\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.2619395852088928 accum:15.716373443603516\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.2554967701435089 accum:15.329805374145508\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.26118794083595276 accum:15.67127513885498\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.26621511578559875 accum:15.972906112670898\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.2659478783607483 accum:15.956871032714844\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2554844915866852 accum:15.329069137573242\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.264963835477829 accum:15.89783000946045\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.2662180960178375 accum:15.973085403442383\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.26800644397735596 accum:16.080385208129883\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.26085951924324036 accum:15.651570320129395\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.25378313660621643 accum:15.226987838745117\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.26598599553108215 accum:15.959158897399902\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2578554153442383 accum:15.471324920654297\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2653980255126953 accum:15.923880577087402\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.2589173913002014 accum:15.535041809082031\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.26607707142829895 accum:15.96462345123291\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.26384204626083374 accum:15.830521583557129\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.2644646465778351 accum:15.867877960205078\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.26261401176452637 accum:15.756840705871582\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.2641969323158264 accum:15.851814270019531\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.27482616901397705 accum:16.48956871032715\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2674190402030945 accum:16.045141220092773\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.26752039790153503 accum:16.051223754882812\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.2683880031108856 accum:16.10327911376953\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.25951898097991943 accum:15.571138381958008\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.2675081491470337 accum:16.050487518310547\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2615245282649994 accum:15.691471099853516\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.26930171251296997 accum:16.15810203552246\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.2708914279937744 accum:16.25348472595215\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.2637239396572113 accum:15.82343578338623\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.2684258222579956 accum:16.105548858642578\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.2724425494670868 accum:16.3465518951416\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.2748074531555176 accum:16.488447189331055\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.2736765742301941 accum:16.42059326171875\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.2668766379356384 accum:16.012598037719727\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2637450098991394 accum:15.824699401855469\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2652782201766968 accum:15.916691780090332\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.2624025046825409 accum:15.744150161743164\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.2656400799751282 accum:15.938404083251953\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2617952227592468 accum:15.70771312713623\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.2702977657318115 accum:16.217864990234375\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2690241038799286 accum:16.14144515991211\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.26863014698028564 accum:16.117807388305664\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.2705913186073303 accum:16.235477447509766\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.27061814069747925 accum:16.23708724975586\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.2750551402568817 accum:16.503307342529297\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.27128657698631287 accum:16.277193069458008\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.28084078431129456 accum:16.850446701049805\n",
      "----------------------------------------\n",
      "sigmoid (7772, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2594374120235443 accum:15.825682640075684\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.27373558282852173 accum:16.697872161865234\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss avg:0.264287531375885 accum:16.121540069580078\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.2601504325866699 accum:15.869176864624023\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.26174837350845337 accum:15.96665096282959\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.26392948627471924 accum:16.099699020385742\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.2611284852027893 accum:15.928837776184082\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.25915008783340454 accum:15.808156967163086\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.26348650455474854 accum:16.072677612304688\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.26097992062568665 accum:15.91977596282959\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.2562441825866699 accum:15.630895614624023\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.24636758863925934 accum:15.028423309326172\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2675936818122864 accum:16.32321548461914\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.26327982544898987 accum:16.060070037841797\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.2613549828529358 accum:15.942655563354492\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.2668868601322174 accum:16.280099868774414\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.263435959815979 accum:16.069595336914062\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.2551587224006653 accum:15.564682960510254\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.2556566894054413 accum:15.595059394836426\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.2574860453605652 accum:15.706649780273438\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2579192519187927 accum:15.733075141906738\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.2608107030391693 accum:15.909454345703125\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.2630806565284729 accum:16.04792022705078\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.2569555342197418 accum:15.674288749694824\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.25162893533706665 accum:15.349366188049316\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.25780656933784485 accum:15.726202011108398\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.2527257204055786 accum:15.41627025604248\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.2558942139148712 accum:15.60954761505127\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2658853530883789 accum:16.21900749206543\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2667909264564514 accum:16.274248123168945\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.26280853152275085 accum:16.031320571899414\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.25789979100227356 accum:15.731888771057129\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.25880444049835205 accum:15.787071228027344\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.2594117820262909 accum:15.824119567871094\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.26060208678245544 accum:15.896728515625\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.2587382197380066 accum:15.783032417297363\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.2640831172466278 accum:16.109071731567383\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.25964969396591187 accum:15.838631629943848\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.25230297446250916 accum:15.390481948852539\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.25543755292892456 accum:15.58169174194336\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.2615007758140564 accum:15.95154857635498\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.26060593128204346 accum:15.89696216583252\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.2572349011898041 accum:15.691329002380371\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.25215885043144226 accum:15.381690979003906\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2583179771900177 accum:15.757397651672363\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.2564021050930023 accum:15.640528678894043\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.2521683871746063 accum:15.382272720336914\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2518725097179413 accum:15.364224433898926\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.26039156317710876 accum:15.883886337280273\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.25400304794311523 accum:15.494187355041504\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.2540847063064575 accum:15.499168395996094\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.2551693320274353 accum:15.565329551696777\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.2551177442073822 accum:15.562182426452637\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.2555953860282898 accum:15.591320037841797\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2661847472190857 accum:16.23727035522461\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.2563199996948242 accum:15.635519981384277\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2634440064430237 accum:16.070085525512695\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.255511611700058 accum:15.586209297180176\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.26441511511802673 accum:16.129322052001953\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.25771263241767883 accum:15.720471382141113\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.25549864768981934 accum:15.585418701171875\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.26209425926208496 accum:15.987751007080078\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.26536688208580017 accum:16.187379837036133\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.25435006618499756 accum:15.515355110168457\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.25600385665893555 accum:15.616235733032227\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2619873285293579 accum:15.981228828430176\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.26505544781684875 accum:16.16838264465332\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.26175981760025024 accum:15.967350006103516\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2605735659599304 accum:15.894989013671875\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.26486143469810486 accum:16.15654754638672\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.25617516040802 accum:15.626686096191406\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2610073387622833 accum:15.92144775390625\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2617231607437134 accum:15.96511459350586\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.2555862069129944 accum:15.59075927734375\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2546299993991852 accum:15.532430648803711\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.2525011897087097 accum:15.402572631835938\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.25258323550224304 accum:15.407578468322754\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.2613278329372406 accum:15.940998077392578\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.2555569112300873 accum:15.588972091674805\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.2553747296333313 accum:15.577858924865723\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.2561369240283966 accum:15.62435245513916\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.259956419467926 accum:15.857342720031738\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.25654470920562744 accum:15.6492280960083\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.24467381834983826 accum:14.925104141235352\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.26425373554229736 accum:16.119478225708008\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.25024446845054626 accum:15.264912605285645\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.259027361869812 accum:15.80066967010498\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.2572217285633087 accum:15.690526008605957\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.26554790139198303 accum:16.198423385620117\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2656954228878021 accum:16.207422256469727\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2567925751209259 accum:15.664347648620605\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.2504067122936249 accum:15.274809837341309\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.2598878741264343 accum:15.853161811828613\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.2528737485408783 accum:15.425299644470215\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.258828341960907 accum:15.788529396057129\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.26173287630081177 accum:15.965706825256348\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.2574957609176636 accum:15.707242012023926\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.25669458508491516 accum:15.658370971679688\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.25617682933807373 accum:15.626787185668945\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.2549351155757904 accum:15.551043510437012\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.258405864238739 accum:15.762758255004883\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.2507546544075012 accum:15.296034812927246\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.24831105768680573 accum:15.14697551727295\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2582760155200958 accum:15.754837036132812\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.2636030614376068 accum:16.079788208007812\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.25899964570999146 accum:15.798978805541992\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.24614471197128296 accum:15.0148286819458\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.25281423330307007 accum:15.421669006347656\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2497352957725525 accum:15.233854293823242\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.26098573207855225 accum:15.920129776000977\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2537621855735779 accum:15.47949504852295\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.24911655485630035 accum:15.196110725402832\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.2525360882282257 accum:15.404702186584473\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.2616446912288666 accum:15.960326194763184\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.258096307516098 accum:15.743875503540039\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.2576451003551483 accum:15.716352462768555\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.2585311830043793 accum:15.770403861999512\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2537808418273926 accum:15.480631828308105\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.25462615489959717 accum:15.532196044921875\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.2504047155380249 accum:15.274687767028809\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.25837722420692444 accum:15.761012077331543\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.26218947768211365 accum:15.993559837341309\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2533765733242035 accum:15.455971717834473\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.25863611698150635 accum:15.776803970336914\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.2542690336704254 accum:15.510412216186523\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.25539374351501465 accum:15.579018592834473\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.24664169549942017 accum:15.045144081115723\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.25064563751220703 accum:15.289384841918945\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.2610759139060974 accum:15.925631523132324\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.25546446442604065 accum:15.583333969116211\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.25244376063346863 accum:15.399070739746094\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.260134756565094 accum:15.868221282958984\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.25649434328079224 accum:15.646156311035156\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.2579888701438904 accum:15.737322807312012\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2608757019042969 accum:15.913418769836426\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.26506248116493225 accum:16.168811798095703\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.26420077681541443 accum:16.116249084472656\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.26157113909721375 accum:15.955841064453125\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.26402243971824646 accum:16.105369567871094\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.27120891213417053 accum:16.543745040893555\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2688107192516327 accum:16.3974552154541\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.27336904406547546 accum:16.675512313842773\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2621627151966095 accum:15.991926193237305\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2530883848667145 accum:15.43839168548584\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.25615644454956055 accum:15.625544548034668\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.2529107332229614 accum:15.427556037902832\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.2508886158466339 accum:15.304206848144531\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.2608683407306671 accum:15.912970542907715\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.2597825527191162 accum:15.846735954284668\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2572575807571411 accum:15.692713737487793\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.25207871198654175 accum:15.376801490783691\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.25300782918930054 accum:15.433478355407715\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.2634558379650116 accum:16.0708065032959\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.2550784945487976 accum:15.559789657592773\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.2504538893699646 accum:15.277688026428223\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.25459203124046326 accum:15.530115127563477\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.25367626547813416 accum:15.47425365447998\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.25917962193489075 accum:15.809957504272461\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2585492730140686 accum:15.771507263183594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2617719769477844 accum:15.96809196472168\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.2555026113986969 accum:15.585660934448242\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.25503313541412354 accum:15.557022094726562\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.2572946548461914 accum:15.694974899291992\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.25730881094932556 accum:15.695838928222656\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.25533822178840637 accum:15.57563304901123\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.24986453354358673 accum:15.241737365722656\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2527601420879364 accum:15.418370246887207\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.2586359977722168 accum:15.7767972946167\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.24475474655628204 accum:14.93004035949707\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.24592024087905884 accum:15.00113582611084\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.25944653153419495 accum:15.826239585876465\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.25804176926612854 accum:15.740549087524414\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.2593790888786316 accum:15.822125434875488\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.25581443309783936 accum:15.604681015014648\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.25978708267211914 accum:15.847013473510742\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.259377121925354 accum:15.822006225585938\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.25485071539878845 accum:15.545894622802734\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.2550899386405945 accum:15.560486793518066\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.255751371383667 accum:15.600834846496582\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2604791820049286 accum:15.889230728149414\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.26490870118141174 accum:16.15943145751953\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.2672651410102844 accum:16.30317497253418\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.26008912920951843 accum:15.865437507629395\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.25967609882354736 accum:15.840242385864258\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.26491814851760864 accum:16.16000747680664\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.2621690630912781 accum:15.992314338684082\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.25031518936157227 accum:15.269227981567383\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.25449201464653015 accum:15.524014472961426\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.24989761412143707 accum:15.243755340576172\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2507544159889221 accum:15.296019554138184\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.25625303387641907 accum:15.63143539428711\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.250628799200058 accum:15.28835678100586\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.24608799815177917 accum:15.011368751525879\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.2606084644794464 accum:15.897116661071777\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2495969980955124 accum:15.225418090820312\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.2529961168766022 accum:15.432764053344727\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.25753670930862427 accum:15.709739685058594\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.25907790660858154 accum:15.803753852844238\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.2525266408920288 accum:15.404125213623047\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.25055503845214844 accum:15.283858299255371\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.25813359022140503 accum:15.746150016784668\n",
      "----------------------------------------\n",
      "isotonic_regression (8340, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2721097767353058 accum:17.959243774414062\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.26718905568122864 accum:17.634477615356445\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.25957784056663513 accum:17.132137298583984\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.2576574683189392 accum:17.00539207458496\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.2694969177246094 accum:17.78679656982422\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2697284519672394 accum:17.802078247070312\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.26601314544677734 accum:17.556867599487305\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.27189528942108154 accum:17.94508934020996\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.27041876316070557 accum:17.847637176513672\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.2619568407535553 accum:17.28915023803711\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.27336347103118896 accum:18.041988372802734\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.271711140871048 accum:17.93293571472168\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2787473499774933 accum:18.397323608398438\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.2748032212257385 accum:18.137012481689453\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.273736834526062 accum:18.066631317138672\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.2633393704891205 accum:17.38039779663086\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.26705828309059143 accum:17.62584686279297\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.2745361626148224 accum:18.119386672973633\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.2719131410121918 accum:17.946266174316406\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.26635080575942993 accum:17.579153060913086\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.27464187145233154 accum:18.12636375427246\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.28655558824539185 accum:18.912668228149414\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.2640433609485626 accum:17.426860809326172\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.26406022906303406 accum:17.427974700927734\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.27323511242866516 accum:18.03351593017578\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.262560099363327 accum:17.32896614074707\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.2680616080760956 accum:17.692066192626953\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.26714566349983215 accum:17.63161277770996\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2876701056957245 accum:18.98622703552246\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2844822406768799 accum:18.775827407836914\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.2697920501232147 accum:17.8062744140625\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.270812451839447 accum:17.873620986938477\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2782047986984253 accum:18.36151695251465\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.2645793557167053 accum:17.462236404418945\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.2765868008136749 accum:18.254728317260742\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.2687353789806366 accum:17.736534118652344\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.2683609127998352 accum:17.711820602416992\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.2712438404560089 accum:17.90209197998047\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.2710913121700287 accum:17.892026901245117\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.26536157727241516 accum:17.51386260986328\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.26964089274406433 accum:17.79629898071289\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.2746712565422058 accum:18.1283016204834\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.27295124530792236 accum:18.014781951904297\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.26216915249824524 accum:17.303163528442383\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2755756378173828 accum:18.187992095947266\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.27662360668182373 accum:18.257158279418945\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.2655857503414154 accum:17.52865982055664\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2713302671909332 accum:17.90779685974121\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.26612892746925354 accum:17.56450843811035\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.26179200410842896 accum:17.27827262878418\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.26268818974494934 accum:17.337419509887695\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.2657713294029236 accum:17.54090690612793\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.26195648312568665 accum:17.289127349853516\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.2596892714500427 accum:17.13949203491211\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.27368226647377014 accum:18.06302833557129\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.2703092694282532 accum:17.840412139892578\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2627909481525421 accum:17.344202041625977\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.2611011266708374 accum:17.23267364501953\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.2568778097629547 accum:16.953935623168945\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.2596266269683838 accum:17.135356903076172\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.2622329592704773 accum:17.307374954223633\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.2655714750289917 accum:17.52771759033203\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2678718864917755 accum:17.67954444885254\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2697390913963318 accum:17.802780151367188\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.2605477273464203 accum:17.196149826049805\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.27356699109077454 accum:18.055419921875\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.2759544253349304 accum:18.21299171447754\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.2735155522823334 accum:18.052026748657227\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.2709333896636963 accum:17.881603240966797\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.2689208984375 accum:17.748779296875\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.27289867401123047 accum:18.01131248474121\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.2678143084049225 accum:17.675743103027344\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2875734269618988 accum:18.97984504699707\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.2791278660297394 accum:18.422439575195312\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2696070373058319 accum:17.794063568115234\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.2757270634174347 accum:18.19798469543457\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.2838139533996582 accum:18.731719970703125\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.2767837643623352 accum:18.267728805541992\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.2749418318271637 accum:18.146160125732422\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.28446394205093384 accum:18.774620056152344\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.28147009015083313 accum:18.5770263671875\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.2786901593208313 accum:18.393550872802734\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.26177552342414856 accum:17.277183532714844\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.25888898968696594 accum:17.086671829223633\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.26059383153915405 accum:17.19919204711914\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.26433491706848145 accum:17.446104049682617\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.261224627494812 accum:17.240825653076172\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.2666047215461731 accum:17.595911026000977\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.26805195212364197 accum:17.691429138183594\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2646149694919586 accum:17.464588165283203\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.27200931310653687 accum:17.952613830566406\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.27396249771118164 accum:18.081523895263672\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.2640872597694397 accum:17.429758071899414\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.26978763937950134 accum:17.805984497070312\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.26677703857421875 accum:17.607284545898438\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.2680223882198334 accum:17.689477920532227\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.2677692472934723 accum:17.67276954650879\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.26189273595809937 accum:17.28491973876953\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.27029943466186523 accum:17.83976173400879\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.2687581777572632 accum:17.738039016723633\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.2577008008956909 accum:17.00825309753418\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.26096031069755554 accum:17.223379135131836\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.25822925567626953 accum:17.04313087463379\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.26256558299064636 accum:17.329328536987305\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.264663428068161 accum:17.467784881591797\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2632848620414734 accum:17.376800537109375\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.2599295675754547 accum:17.155351638793945\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2607797384262085 accum:17.211462020874023\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2600958049297333 accum:17.166322708129883\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.2631414532661438 accum:17.36733627319336\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2623787224292755 accum:17.31699562072754\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.25490009784698486 accum:16.823406219482422\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.2643946707248688 accum:17.450048446655273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.26589682698249817 accum:17.549190521240234\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2716938555240631 accum:17.931793212890625\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.26277080178260803 accum:17.342872619628906\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.27485254406929016 accum:18.14026641845703\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.28263938426971436 accum:18.654199600219727\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.27097126841545105 accum:17.884103775024414\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.2771620750427246 accum:18.292695999145508\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.2611635625362396 accum:17.23679542541504\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.258059024810791 accum:17.03189468383789\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2742447555065155 accum:18.10015296936035\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.25832733511924744 accum:17.049604415893555\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.25674042105674744 accum:16.944868087768555\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.2663172781467438 accum:17.576940536499023\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.27476656436920166 accum:18.134592056274414\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.26896241307258606 accum:17.75151824951172\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.2633143663406372 accum:17.378747940063477\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.26633837819099426 accum:17.578332901000977\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.2707570493221283 accum:17.869964599609375\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.2597806751728058 accum:17.145523071289062\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.2609182298183441 accum:17.22060203552246\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.2635202705860138 accum:17.392337799072266\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2600521147251129 accum:17.16343879699707\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.264132559299469 accum:17.432748794555664\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.251308798789978 accum:16.586380004882812\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.266191303730011 accum:17.568626403808594\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.26718977093696594 accum:17.634525299072266\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.25869306921958923 accum:17.073741912841797\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.2602657377719879 accum:17.17753791809082\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.26640182733535767 accum:17.58251953125\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2623026371002197 accum:17.311973571777344\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2562476396560669 accum:16.912343978881836\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.25915855169296265 accum:17.104463577270508\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.26307839155197144 accum:17.36317253112793\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.26242852210998535 accum:17.320281982421875\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.2613319158554077 accum:17.247905731201172\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.25516149401664734 accum:16.84065818786621\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.25725656747817993 accum:16.978933334350586\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.26439422369003296 accum:17.45001792907715\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.2607715129852295 accum:17.210918426513672\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.26237672567367554 accum:17.316864013671875\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.2656368613243103 accum:17.532032012939453\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.2706431746482849 accum:17.862449645996094\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.2853887975215912 accum:18.835660934448242\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.27444881200790405 accum:18.11362075805664\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.2657363712787628 accum:17.53860092163086\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2677086889743805 accum:17.668773651123047\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2712237238883972 accum:17.90076446533203\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.2669490873813629 accum:17.61863899230957\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.262532114982605 accum:17.327119827270508\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.2601320743560791 accum:17.168716430664062\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.25663962960243225 accum:16.938215255737305\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.2613213360309601 accum:17.247207641601562\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.25732842087745667 accum:16.983675003051758\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.26568856835365295 accum:17.535444259643555\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.2561049461364746 accum:16.902925491333008\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2533462941646576 accum:16.720855712890625\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.2535727322101593 accum:16.73579978942871\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.25853416323661804 accum:17.06325340270996\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.26975592970848083 accum:17.803890228271484\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.25821077823638916 accum:17.04191017150879\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.27346572279930115 accum:18.048736572265625\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2783605456352234 accum:18.371795654296875\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.26799342036247253 accum:17.687564849853516\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.25947412848472595 accum:17.12529182434082\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.2605130970478058 accum:17.193864822387695\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.26290544867515564 accum:17.35175895690918\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2573626935482025 accum:16.985937118530273\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.2613137662410736 accum:17.246707916259766\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.272815078496933 accum:18.005794525146484\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.26397937536239624 accum:17.422637939453125\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.27430108189582825 accum:18.103870391845703\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.2694040834903717 accum:17.780668258666992\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.267016738653183 accum:17.623104095458984\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.26841211318969727 accum:17.715198516845703\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.2715461552143097 accum:17.92204475402832\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2603169083595276 accum:17.18091583251953\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.25572657585144043 accum:16.877954483032227\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.25593101978302 accum:16.891447067260742\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.26680120825767517 accum:17.60887908935547\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2683064937591553 accum:17.708227157592773\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.26964738965034485 accum:17.79672622680664\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.261422336101532 accum:17.253873825073242\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.259356826543808 accum:17.117549896240234\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.2532987594604492 accum:16.71771812438965\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.26285627484321594 accum:17.348512649536133\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.2547684609889984 accum:16.81471824645996\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.26548343896865845 accum:17.52190589904785\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.2642919421195984 accum:17.443267822265625\n",
      "----------------------------------------\n",
      "zscore (7698, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.26010414958000183 accum:15.866353988647461\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.26933011412620544 accum:16.42913818359375\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.2632015347480774 accum:16.055294036865234\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.27791014313697815 accum:16.9525203704834\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.26324549317359924 accum:16.05797576904297\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2539413571357727 accum:15.490423202514648\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.25391265749931335 accum:15.488673210144043\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.26083314418792725 accum:15.910822868347168\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.2594800591468811 accum:15.82828426361084\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.2631247937679291 accum:16.050613403320312\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.26279810070991516 accum:16.030685424804688\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.2637672424316406 accum:16.089801788330078\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2627995014190674 accum:16.030771255493164\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.2596524953842163 accum:15.8388032913208\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.2619827091693878 accum:15.980945587158203\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.26415351033210754 accum:16.113365173339844\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.2590988874435425 accum:15.805033683776855\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.2569587826728821 accum:15.67448616027832\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.25882160663604736 accum:15.788119316101074\n",
      "----------------------------------------\n",
      "epoch: 19 loss avg:0.26250553131103516 accum:16.01283836364746\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.2726219594478607 accum:16.629940032958984\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.2573779821395874 accum:15.700057983398438\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.25304949283599854 accum:15.436019897460938\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.2528221309185028 accum:15.422150611877441\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.24838252365589142 accum:15.151334762573242\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.2554718554019928 accum:15.583784103393555\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.2643861174583435 accum:16.127553939819336\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.26223981380462646 accum:15.996628761291504\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.2553490996360779 accum:15.576295852661133\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2564869523048401 accum:15.64570426940918\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.2509230375289917 accum:15.306305885314941\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.2612156271934509 accum:15.93415355682373\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.2581682503223419 accum:15.74826431274414\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.2638203799724579 accum:16.09304428100586\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.26097697019577026 accum:15.919595718383789\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.25661101937294006 accum:15.653273582458496\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.25336509943008423 accum:15.455272674560547\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.2565566897392273 accum:15.649959564208984\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.2550887167453766 accum:15.560413360595703\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.2490977644920349 accum:15.194964408874512\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.255530446767807 accum:15.587357521057129\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.25613895058631897 accum:15.624476432800293\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.25924307107925415 accum:15.813828468322754\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.2589160203933716 accum:15.793878555297852\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2661426365375519 accum:16.23470115661621\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.2616112530231476 accum:15.958287239074707\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.26909345388412476 accum:16.414701461791992\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2626849412918091 accum:16.02378273010254\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.26033565402030945 accum:15.880475044250488\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.2584086060523987 accum:15.762925148010254\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.2600327730178833 accum:15.86199951171875\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.2640954256057739 accum:16.109821319580078\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.26163387298583984 accum:15.959667205810547\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.2573421895503998 accum:15.697874069213867\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2539816200733185 accum:15.492879867553711\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.24695999920368195 accum:15.064560890197754\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.25122183561325073 accum:15.324533462524414\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.259461909532547 accum:15.827178001403809\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.25230956077575684 accum:15.390883445739746\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.25509053468704224 accum:15.56052303314209\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.25177237391471863 accum:15.358116149902344\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.25637415051460266 accum:15.638824462890625\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2560740113258362 accum:15.620514869689941\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2578318119049072 accum:15.727742195129395\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.25073155760765076 accum:15.294625282287598\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2533870339393616 accum:15.456609725952148\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 66 loss avg:0.254054456949234 accum:15.497322082519531\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.26091545820236206 accum:15.91584300994873\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.26630261540412903 accum:16.244461059570312\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.2574194669723511 accum:15.70258903503418\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.2588873505592346 accum:15.792128562927246\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.25554779171943665 accum:15.58841609954834\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.2596791684627533 accum:15.84043025970459\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.25930294394493103 accum:15.817480087280273\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.26157718896865845 accum:15.956209182739258\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.26580294966697693 accum:16.21398162841797\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.2595054805278778 accum:15.829835891723633\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.2550879716873169 accum:15.560367584228516\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.25116392970085144 accum:15.321000099182129\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.24859783053398132 accum:15.164468765258789\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.24923814833164215 accum:15.203527450561523\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.25460928678512573 accum:15.531167984008789\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.26651206612586975 accum:16.25723648071289\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.24520400166511536 accum:14.95744514465332\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.24945321679115295 accum:15.216647148132324\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.24565191566944122 accum:14.98476791381836\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.2560758590698242 accum:15.620627403259277\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.2538900375366211 accum:15.487293243408203\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.2560445964336395 accum:15.618721961975098\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2652362585067749 accum:16.179412841796875\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.25840333104133606 accum:15.762604713439941\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.24721579253673553 accum:15.080163955688477\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.25600510835647583 accum:15.616312026977539\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.24571308493614197 accum:14.98849868774414\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.24560049176216125 accum:14.9816312789917\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.24857880175113678 accum:15.163308143615723\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.25115084648132324 accum:15.320202827453613\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.2635653614997864 accum:16.07748794555664\n",
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.25595203042030334 accum:15.61307430267334\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.25050830841064453 accum:15.281006813049316\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.25493648648262024 accum:15.551126480102539\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.2505527138710022 accum:15.283717155456543\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.25543153285980225 accum:15.581324577331543\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.2587013840675354 accum:15.780784606933594\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.25609371066093445 accum:15.621716499328613\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2526206970214844 accum:15.409863471984863\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.2500419020652771 accum:15.252557754516602\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.26909399032592773 accum:16.41473388671875\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.2611507773399353 accum:15.930198669433594\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.254194438457489 accum:15.505861282348633\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.2565336227416992 accum:15.648550987243652\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.25502049922943115 accum:15.55625057220459\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.25164103507995605 accum:15.350103378295898\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.24394741654396057 accum:14.880793571472168\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2526419758796692 accum:15.411161422729492\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.2569117248058319 accum:15.671615600585938\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.25333330035209656 accum:15.45333194732666\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2530195116996765 accum:15.43419075012207\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.25340595841407776 accum:15.457764625549316\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.2673480808734894 accum:16.3082332611084\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.2586676776409149 accum:15.778729438781738\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.26625150442123413 accum:16.241342544555664\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2671324610710144 accum:16.295080184936523\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.2507908344268799 accum:15.29824161529541\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.25535494089126587 accum:15.576651573181152\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.250981867313385 accum:15.309894561767578\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.26164519786834717 accum:15.960357666015625\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2613077163696289 accum:15.93977165222168\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.25650453567504883 accum:15.646778106689453\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.25678905844688416 accum:15.66413402557373\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.25133442878723145 accum:15.331400871276855\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.24485433101654053 accum:14.936115264892578\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.2540005147457123 accum:15.49403190612793\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.2569950222969055 accum:15.676697731018066\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2576432526111603 accum:15.716238975524902\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.26118627190589905 accum:15.932363510131836\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.2563178539276123 accum:15.635390281677246\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.252605140209198 accum:15.408914566040039\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.2628951370716095 accum:16.036603927612305\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.2527885138988495 accum:15.420100212097168\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.24598965048789978 accum:15.005369186401367\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.24716141819953918 accum:15.076847076416016\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.24945518374443054 accum:15.216767311096191\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2483908236026764 accum:15.151841163635254\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.252996563911438 accum:15.432790756225586\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.2524343430995941 accum:15.3984956741333\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.2617640495300293 accum:15.967608451843262\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.261144757270813 accum:15.929830551147461\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.25706788897514343 accum:15.681142807006836\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2534272074699402 accum:15.459060668945312\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.24736014008522034 accum:15.088969230651855\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.26112842559814453 accum:15.928833961486816\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.25546208024024963 accum:15.583187103271484\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.24797363579273224 accum:15.126392364501953\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.2504601776599884 accum:15.278072357177734\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.2517916262149811 accum:15.35929012298584\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.25176408886909485 accum:15.357609748840332\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.2520953118801117 accum:15.377815246582031\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.25176921486854553 accum:15.35792350769043\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.2510829269886017 accum:15.316059112548828\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.24779994785785675 accum:15.115797996520996\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.2474089413881302 accum:15.091946601867676\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.24338504672050476 accum:14.846488952636719\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.2589370906352997 accum:15.79516315460205\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.2624533772468567 accum:16.00965690612793\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2523620128631592 accum:15.394083023071289\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2510443925857544 accum:15.313709259033203\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.25547486543655396 accum:15.583968162536621\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2556239068508148 accum:15.593058586120605\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.26051580905914307 accum:15.891465187072754\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.2582744061946869 accum:15.754738807678223\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.24546970427036285 accum:14.973652839660645\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.25716036558151245 accum:15.686782836914062\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.2507198452949524 accum:15.293911933898926\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2569858133792877 accum:15.676136016845703\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.2515728175640106 accum:15.345942497253418\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.2519034147262573 accum:15.366108894348145\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.25090292096138 accum:15.305079460144043\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.24296699464321136 accum:14.820987701416016\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.24117720127105713 accum:14.711810111999512\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.2518691420555115 accum:15.364018440246582\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.2612164318561554 accum:15.934203147888184\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.2607494294643402 accum:15.905715942382812\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.26322606205940247 accum:16.056791305541992\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.26830098032951355 accum:16.366361618041992\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.2620117962360382 accum:15.982720375061035\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.26472967863082886 accum:16.14851188659668\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.2557571828365326 accum:15.601188659667969\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2511281967163086 accum:15.318819999694824\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.24662946164608002 accum:15.044398307800293\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.24981564283370972 accum:15.238755226135254\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.24823148548603058 accum:15.142121315002441\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2535764276981354 accum:15.46816349029541\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.24837084114551544 accum:15.150622367858887\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.24962909519672394 accum:15.227375984191895\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.24882280826568604 accum:15.178192138671875\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.24423335492610931 accum:14.898235321044922\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2574796676635742 accum:15.706259727478027\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.24800865352153778 accum:15.128528594970703\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.24785055220127106 accum:15.118884086608887\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.2527485191822052 accum:15.417659759521484\n",
      "----------------------------------------\n",
      "normalize (7734, 400)\n",
      "--------------------------------------------------\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.0001\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "epoch: 0 loss avg:0.2751712203025818 accum:16.785446166992188\n",
      "----------------------------------------\n",
      "epoch: 1 loss avg:0.2640482187271118 accum:16.106943130493164\n",
      "----------------------------------------\n",
      "epoch: 2 loss avg:0.26000601053237915 accum:15.860367774963379\n",
      "----------------------------------------\n",
      "epoch: 3 loss avg:0.2615746557712555 accum:15.956055641174316\n",
      "----------------------------------------\n",
      "epoch: 4 loss avg:0.26589280366897583 accum:16.21946144104004\n",
      "----------------------------------------\n",
      "epoch: 5 loss avg:0.2606043219566345 accum:15.89686393737793\n",
      "----------------------------------------\n",
      "epoch: 6 loss avg:0.2571614682674408 accum:15.686849594116211\n",
      "----------------------------------------\n",
      "epoch: 7 loss avg:0.2567095458507538 accum:15.659283638000488\n",
      "----------------------------------------\n",
      "epoch: 8 loss avg:0.26055487990379333 accum:15.89384937286377\n",
      "----------------------------------------\n",
      "epoch: 9 loss avg:0.26033881306648254 accum:15.880668640136719\n",
      "----------------------------------------\n",
      "epoch: 10 loss avg:0.2649449110031128 accum:16.161640167236328\n",
      "----------------------------------------\n",
      "epoch: 11 loss avg:0.25833699107170105 accum:15.758557319641113\n",
      "----------------------------------------\n",
      "epoch: 12 loss avg:0.2623005509376526 accum:16.000333786010742\n",
      "----------------------------------------\n",
      "epoch: 13 loss avg:0.26389366388320923 accum:16.097515106201172\n",
      "----------------------------------------\n",
      "epoch: 14 loss avg:0.25537922978401184 accum:15.578134536743164\n",
      "----------------------------------------\n",
      "epoch: 15 loss avg:0.26192784309387207 accum:15.97760009765625\n",
      "----------------------------------------\n",
      "epoch: 16 loss avg:0.26662248373031616 accum:16.263973236083984\n",
      "----------------------------------------\n",
      "epoch: 17 loss avg:0.26625755429267883 accum:16.24171257019043\n",
      "----------------------------------------\n",
      "epoch: 18 loss avg:0.25923648476600647 accum:15.81342601776123\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 loss avg:0.25203198194503784 accum:15.37395191192627\n",
      "----------------------------------------\n",
      "epoch: 20 loss avg:0.25428062677383423 accum:15.51111888885498\n",
      "----------------------------------------\n",
      "epoch: 21 loss avg:0.259798139333725 accum:15.847686767578125\n",
      "----------------------------------------\n",
      "epoch: 22 loss avg:0.26386165618896484 accum:16.095561981201172\n",
      "----------------------------------------\n",
      "epoch: 23 loss avg:0.2581333518028259 accum:15.746134757995605\n",
      "----------------------------------------\n",
      "epoch: 24 loss avg:0.258043110370636 accum:15.740631103515625\n",
      "----------------------------------------\n",
      "epoch: 25 loss avg:0.25458669662475586 accum:15.529788970947266\n",
      "----------------------------------------\n",
      "epoch: 26 loss avg:0.2516957223415375 accum:15.353439331054688\n",
      "----------------------------------------\n",
      "epoch: 27 loss avg:0.258566677570343 accum:15.772567749023438\n",
      "----------------------------------------\n",
      "epoch: 28 loss avg:0.25030532479286194 accum:15.268625259399414\n",
      "----------------------------------------\n",
      "epoch: 29 loss avg:0.2583343982696533 accum:15.758399963378906\n",
      "----------------------------------------\n",
      "epoch: 30 loss avg:0.24747397005558014 accum:15.09591293334961\n",
      "----------------------------------------\n",
      "epoch: 31 loss avg:0.24889372289180756 accum:15.182518005371094\n",
      "----------------------------------------\n",
      "epoch: 32 loss avg:0.250227153301239 accum:15.263856887817383\n",
      "----------------------------------------\n",
      "epoch: 33 loss avg:0.2526833415031433 accum:15.413683891296387\n",
      "----------------------------------------\n",
      "epoch: 34 loss avg:0.25518813729286194 accum:15.566476821899414\n",
      "----------------------------------------\n",
      "epoch: 35 loss avg:0.25980281829833984 accum:15.847972869873047\n",
      "----------------------------------------\n",
      "epoch: 36 loss avg:0.2559574544429779 accum:15.613405227661133\n",
      "----------------------------------------\n",
      "epoch: 37 loss avg:0.251952588558197 accum:15.369109153747559\n",
      "----------------------------------------\n",
      "epoch: 38 loss avg:0.2542573809623718 accum:15.509700775146484\n",
      "----------------------------------------\n",
      "epoch: 39 loss avg:0.25411874055862427 accum:15.501243591308594\n",
      "----------------------------------------\n",
      "epoch: 40 loss avg:0.2591131329536438 accum:15.805902481079102\n",
      "----------------------------------------\n",
      "epoch: 41 loss avg:0.26163002848625183 accum:15.959432601928711\n",
      "----------------------------------------\n",
      "epoch: 42 loss avg:0.2606724798679352 accum:15.901022911071777\n",
      "----------------------------------------\n",
      "epoch: 43 loss avg:0.26261499524116516 accum:16.019515991210938\n",
      "----------------------------------------\n",
      "epoch: 44 loss avg:0.2623826265335083 accum:16.005340576171875\n",
      "----------------------------------------\n",
      "epoch: 45 loss avg:0.25659531354904175 accum:15.652315139770508\n",
      "----------------------------------------\n",
      "epoch: 46 loss avg:0.25409775972366333 accum:15.499964714050293\n",
      "----------------------------------------\n",
      "epoch: 47 loss avg:0.2519097924232483 accum:15.366498947143555\n",
      "----------------------------------------\n",
      "epoch: 48 loss avg:0.25783148407936096 accum:15.727721214294434\n",
      "----------------------------------------\n",
      "epoch: 49 loss avg:0.25215232372283936 accum:15.381292343139648\n",
      "----------------------------------------\n",
      "epoch: 50 loss avg:0.24557730555534363 accum:14.980216026306152\n",
      "----------------------------------------\n",
      "epoch: 51 loss avg:0.26093924045562744 accum:15.917295455932617\n",
      "----------------------------------------\n",
      "epoch: 52 loss avg:0.26003459095954895 accum:15.862110137939453\n",
      "----------------------------------------\n",
      "epoch: 53 loss avg:0.25817346572875977 accum:15.74858283996582\n",
      "----------------------------------------\n",
      "epoch: 54 loss avg:0.2569121718406677 accum:15.671643257141113\n",
      "----------------------------------------\n",
      "epoch: 55 loss avg:0.25431105494499207 accum:15.512975692749023\n",
      "----------------------------------------\n",
      "epoch: 56 loss avg:0.2520531713962555 accum:15.375244140625\n",
      "----------------------------------------\n",
      "epoch: 57 loss avg:0.2566118836402893 accum:15.653326034545898\n",
      "----------------------------------------\n",
      "epoch: 58 loss avg:0.25283098220825195 accum:15.422690391540527\n",
      "----------------------------------------\n",
      "epoch: 59 loss avg:0.2542409300804138 accum:15.508697509765625\n",
      "----------------------------------------\n",
      "epoch: 60 loss avg:0.2551395893096924 accum:15.563515663146973\n",
      "----------------------------------------\n",
      "epoch: 61 loss avg:0.2570829391479492 accum:15.682059288024902\n",
      "----------------------------------------\n",
      "epoch: 62 loss avg:0.2576235830783844 accum:15.71504020690918\n",
      "----------------------------------------\n",
      "epoch: 63 loss avg:0.2484373301267624 accum:15.154678344726562\n",
      "----------------------------------------\n",
      "epoch: 64 loss avg:0.2576310336589813 accum:15.715493202209473\n",
      "----------------------------------------\n",
      "epoch: 65 loss avg:0.2653442919254303 accum:16.186002731323242\n",
      "----------------------------------------\n",
      "epoch: 66 loss avg:0.24695022404193878 accum:15.06396484375\n",
      "----------------------------------------\n",
      "epoch: 67 loss avg:0.2563389837741852 accum:15.636679649353027\n",
      "----------------------------------------\n",
      "epoch: 68 loss avg:0.25599098205566406 accum:15.615449905395508\n",
      "----------------------------------------\n",
      "epoch: 69 loss avg:0.25564953684806824 accum:15.594622611999512\n",
      "----------------------------------------\n",
      "epoch: 70 loss avg:0.25469616055488586 accum:15.536467552185059\n",
      "----------------------------------------\n",
      "epoch: 71 loss avg:0.25255343317985535 accum:15.405760765075684\n",
      "----------------------------------------\n",
      "epoch: 72 loss avg:0.26461589336395264 accum:16.141571044921875\n",
      "----------------------------------------\n",
      "epoch: 73 loss avg:0.2670561671257019 accum:16.29042625427246\n",
      "----------------------------------------\n",
      "epoch: 74 loss avg:0.2497907280921936 accum:15.237235069274902\n",
      "----------------------------------------\n",
      "epoch: 75 loss avg:0.2540103495121002 accum:15.494632720947266\n",
      "----------------------------------------\n",
      "epoch: 76 loss avg:0.2613924741744995 accum:15.944942474365234\n",
      "----------------------------------------\n",
      "epoch: 77 loss avg:0.25834405422210693 accum:15.758987426757812\n",
      "----------------------------------------\n",
      "epoch: 78 loss avg:0.25433146953582764 accum:15.51422119140625\n",
      "----------------------------------------\n",
      "epoch: 79 loss avg:0.2567450702190399 accum:15.661449432373047\n",
      "----------------------------------------\n",
      "epoch: 80 loss avg:0.2518731653690338 accum:15.364264488220215\n",
      "----------------------------------------\n",
      "epoch: 81 loss avg:0.25275787711143494 accum:15.418231964111328\n",
      "----------------------------------------\n",
      "epoch: 82 loss avg:0.24765753746032715 accum:15.107110977172852\n",
      "----------------------------------------\n",
      "epoch: 83 loss avg:0.25560829043388367 accum:15.592106819152832\n",
      "----------------------------------------\n",
      "epoch: 84 loss avg:0.2649157643318176 accum:16.159862518310547\n",
      "----------------------------------------\n",
      "epoch: 85 loss avg:0.2582326829433441 accum:15.75219440460205\n",
      "----------------------------------------\n",
      "epoch: 86 loss avg:0.2607724666595459 accum:15.907121658325195\n",
      "----------------------------------------\n",
      "epoch: 87 loss avg:0.24959653615951538 accum:15.22538948059082\n",
      "----------------------------------------\n",
      "epoch: 88 loss avg:0.24664226174354553 accum:15.045178413391113\n",
      "----------------------------------------\n",
      "epoch: 89 loss avg:0.2532380223274231 accum:15.44752025604248\n",
      "----------------------------------------\n",
      "epoch: 90 loss avg:0.2591467797756195 accum:15.807953834533691\n",
      "----------------------------------------\n",
      "epoch: 91 loss avg:0.2481822818517685 accum:15.139120101928711\n",
      "----------------------------------------\n",
      "epoch: 92 loss avg:0.25191837549209595 accum:15.367022514343262\n",
      "----------------------------------------\n",
      "epoch: 93 loss avg:0.2507690191268921 accum:15.296911239624023\n",
      "----------------------------------------\n",
      "epoch: 94 loss avg:0.24823324382305145 accum:15.142229080200195\n",
      "----------------------------------------\n",
      "epoch: 95 loss avg:0.25257906317710876 accum:15.407322883605957\n",
      "----------------------------------------\n",
      "epoch: 96 loss avg:0.24886174499988556 accum:15.180567741394043\n",
      "----------------------------------------\n",
      "epoch: 97 loss avg:0.25235214829444885 accum:15.393482208251953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "epoch: 98 loss avg:0.2525618076324463 accum:15.406271934509277\n",
      "----------------------------------------\n",
      "epoch: 99 loss avg:0.25270983576774597 accum:15.415301322937012\n",
      "----------------------------------------\n",
      "epoch: 100 loss avg:0.2494983822107315 accum:15.219402313232422\n",
      "----------------------------------------\n",
      "epoch: 101 loss avg:0.24732163548469543 accum:15.086620330810547\n",
      "----------------------------------------\n",
      "epoch: 102 loss avg:0.26063793897628784 accum:15.898914337158203\n",
      "----------------------------------------\n",
      "epoch: 103 loss avg:0.25750958919525146 accum:15.708085060119629\n",
      "----------------------------------------\n",
      "epoch: 104 loss avg:0.25613394379615784 accum:15.624171257019043\n",
      "----------------------------------------\n",
      "epoch: 105 loss avg:0.2562674283981323 accum:15.63231372833252\n",
      "----------------------------------------\n",
      "epoch: 106 loss avg:0.2519638240337372 accum:15.369794845581055\n",
      "----------------------------------------\n",
      "epoch: 107 loss avg:0.2505388557910919 accum:15.28287124633789\n",
      "----------------------------------------\n",
      "epoch: 108 loss avg:0.25940853357315063 accum:15.823922157287598\n",
      "----------------------------------------\n",
      "epoch: 109 loss avg:0.2612677812576294 accum:15.937335014343262\n",
      "----------------------------------------\n",
      "epoch: 110 loss avg:0.25764307379722595 accum:15.716228485107422\n",
      "----------------------------------------\n",
      "epoch: 111 loss avg:0.25590065121650696 accum:15.609941482543945\n",
      "----------------------------------------\n",
      "epoch: 112 loss avg:0.25788962841033936 accum:15.731268882751465\n",
      "----------------------------------------\n",
      "epoch: 113 loss avg:0.2589316666126251 accum:15.794833183288574\n",
      "----------------------------------------\n",
      "epoch: 114 loss avg:0.2603405714035034 accum:15.880776405334473\n",
      "----------------------------------------\n",
      "epoch: 115 loss avg:0.25572866201400757 accum:15.599449157714844\n",
      "----------------------------------------\n",
      "epoch: 116 loss avg:0.2481359988451004 accum:15.136297225952148\n",
      "----------------------------------------\n",
      "epoch: 117 loss avg:0.2556561827659607 accum:15.5950288772583\n",
      "----------------------------------------\n",
      "epoch: 118 loss avg:0.25525522232055664 accum:15.57056999206543\n",
      "----------------------------------------\n",
      "epoch: 119 loss avg:0.25444528460502625 accum:15.521162986755371\n",
      "----------------------------------------\n",
      "epoch: 120 loss avg:0.2587904632091522 accum:15.786219596862793\n",
      "----------------------------------------\n",
      "epoch: 121 loss avg:0.2593894302845001 accum:15.82275676727295\n",
      "----------------------------------------\n",
      "epoch: 122 loss avg:0.2522222697734833 accum:15.38555908203125\n",
      "----------------------------------------\n",
      "epoch: 123 loss avg:0.25023797154426575 accum:15.264517784118652\n",
      "----------------------------------------\n",
      "epoch: 124 loss avg:0.2612048387527466 accum:15.93349552154541\n",
      "----------------------------------------\n",
      "epoch: 125 loss avg:0.26064956188201904 accum:15.89962387084961\n",
      "----------------------------------------\n",
      "epoch: 126 loss avg:0.2583930492401123 accum:15.761977195739746\n",
      "----------------------------------------\n",
      "epoch: 127 loss avg:0.2569788694381714 accum:15.675711631774902\n",
      "----------------------------------------\n",
      "epoch: 128 loss avg:0.2530842423439026 accum:15.438138961791992\n",
      "----------------------------------------\n",
      "epoch: 129 loss avg:0.25963887572288513 accum:15.837971687316895\n",
      "----------------------------------------\n",
      "epoch: 130 loss avg:0.2554691731929779 accum:15.583620071411133\n",
      "----------------------------------------\n",
      "epoch: 131 loss avg:0.25475409626960754 accum:15.540000915527344\n",
      "----------------------------------------\n",
      "epoch: 132 loss avg:0.2575209438800812 accum:15.708778381347656\n",
      "----------------------------------------\n",
      "epoch: 133 loss avg:0.2555080056190491 accum:15.585988998413086\n",
      "----------------------------------------\n",
      "epoch: 134 loss avg:0.2570212185382843 accum:15.678295135498047\n",
      "----------------------------------------\n",
      "epoch: 135 loss avg:0.25981369614601135 accum:15.848636627197266\n",
      "----------------------------------------\n",
      "epoch: 136 loss avg:0.2538931369781494 accum:15.487483024597168\n",
      "----------------------------------------\n",
      "epoch: 137 loss avg:0.25630515813827515 accum:15.634615898132324\n",
      "----------------------------------------\n",
      "epoch: 138 loss avg:0.2631183862686157 accum:16.050222396850586\n",
      "----------------------------------------\n",
      "epoch: 139 loss avg:0.2575380206108093 accum:15.709819793701172\n",
      "----------------------------------------\n",
      "epoch: 140 loss avg:0.25610917806625366 accum:15.622661590576172\n",
      "----------------------------------------\n",
      "epoch: 141 loss avg:0.2549743950366974 accum:15.553439140319824\n",
      "----------------------------------------\n",
      "epoch: 142 loss avg:0.2537866234779358 accum:15.480984687805176\n",
      "----------------------------------------\n",
      "epoch: 143 loss avg:0.2540697157382965 accum:15.49825382232666\n",
      "----------------------------------------\n",
      "epoch: 144 loss avg:0.2529289424419403 accum:15.428667068481445\n",
      "----------------------------------------\n",
      "epoch: 145 loss avg:0.261980801820755 accum:15.980830192565918\n",
      "----------------------------------------\n",
      "epoch: 146 loss avg:0.2616039514541626 accum:15.957842826843262\n",
      "----------------------------------------\n",
      "epoch: 147 loss avg:0.2589266300201416 accum:15.794525146484375\n",
      "----------------------------------------\n",
      "epoch: 148 loss avg:0.25426429510116577 accum:15.510123252868652\n",
      "----------------------------------------\n",
      "epoch: 149 loss avg:0.2590489685535431 accum:15.801987648010254\n",
      "----------------------------------------\n",
      "epoch: 150 loss avg:0.24823059141635895 accum:15.142066955566406\n",
      "----------------------------------------\n",
      "epoch: 151 loss avg:0.2536497712135315 accum:15.472636222839355\n",
      "----------------------------------------\n",
      "epoch: 152 loss avg:0.25670039653778076 accum:15.658724784851074\n",
      "----------------------------------------\n",
      "epoch: 153 loss avg:0.259943425655365 accum:15.856549263000488\n",
      "----------------------------------------\n",
      "epoch: 154 loss avg:0.2514708340167999 accum:15.3397216796875\n",
      "----------------------------------------\n",
      "epoch: 155 loss avg:0.2522553503513336 accum:15.387577056884766\n",
      "----------------------------------------\n",
      "epoch: 156 loss avg:0.25690576434135437 accum:15.671253204345703\n",
      "----------------------------------------\n",
      "epoch: 157 loss avg:0.26026424765586853 accum:15.876119613647461\n",
      "----------------------------------------\n",
      "epoch: 158 loss avg:0.2524885833263397 accum:15.401803970336914\n",
      "----------------------------------------\n",
      "epoch: 159 loss avg:0.24458523094654083 accum:14.919699668884277\n",
      "----------------------------------------\n",
      "epoch: 160 loss avg:0.25332126021385193 accum:15.452597618103027\n",
      "----------------------------------------\n",
      "epoch: 161 loss avg:0.24817633628845215 accum:15.138757705688477\n",
      "----------------------------------------\n",
      "epoch: 162 loss avg:0.24943706393241882 accum:15.215662002563477\n",
      "----------------------------------------\n",
      "epoch: 163 loss avg:0.25582948327064514 accum:15.605599403381348\n",
      "----------------------------------------\n",
      "epoch: 164 loss avg:0.2562599182128906 accum:15.631855010986328\n",
      "----------------------------------------\n",
      "epoch: 165 loss avg:0.2463388442993164 accum:15.026670455932617\n",
      "----------------------------------------\n",
      "epoch: 166 loss avg:0.2513318657875061 accum:15.331244468688965\n",
      "----------------------------------------\n",
      "epoch: 167 loss avg:0.25147852301597595 accum:15.340189933776855\n",
      "----------------------------------------\n",
      "epoch: 168 loss avg:0.2566984295845032 accum:15.658604621887207\n",
      "----------------------------------------\n",
      "epoch: 169 loss avg:0.25857800245285034 accum:15.773258209228516\n",
      "----------------------------------------\n",
      "epoch: 170 loss avg:0.25980493426322937 accum:15.848102569580078\n",
      "----------------------------------------\n",
      "epoch: 171 loss avg:0.2600131332874298 accum:15.86080265045166\n",
      "----------------------------------------\n",
      "epoch: 172 loss avg:0.24818803369998932 accum:15.139471054077148\n",
      "----------------------------------------\n",
      "epoch: 173 loss avg:0.25139304995536804 accum:15.334976196289062\n",
      "----------------------------------------\n",
      "epoch: 174 loss avg:0.2505948543548584 accum:15.286286354064941\n",
      "----------------------------------------\n",
      "epoch: 175 loss avg:0.26373082399368286 accum:16.087581634521484\n",
      "----------------------------------------\n",
      "epoch: 176 loss avg:0.2574031352996826 accum:15.701592445373535\n",
      "----------------------------------------\n",
      "epoch: 177 loss avg:0.25026562809944153 accum:15.266204833984375\n",
      "----------------------------------------\n",
      "epoch: 178 loss avg:0.2507998049259186 accum:15.298788070678711\n",
      "----------------------------------------\n",
      "epoch: 179 loss avg:0.2568278908729553 accum:15.666501998901367\n",
      "----------------------------------------\n",
      "epoch: 180 loss avg:0.24604883790016174 accum:15.008979797363281\n",
      "----------------------------------------\n",
      "epoch: 181 loss avg:0.24921981990337372 accum:15.202409744262695\n",
      "----------------------------------------\n",
      "epoch: 182 loss avg:0.2575302720069885 accum:15.709346771240234\n",
      "----------------------------------------\n",
      "epoch: 183 loss avg:0.25175586342811584 accum:15.357109069824219\n",
      "----------------------------------------\n",
      "epoch: 184 loss avg:0.2550762891769409 accum:15.559654235839844\n",
      "----------------------------------------\n",
      "epoch: 185 loss avg:0.251049280166626 accum:15.314007759094238\n",
      "----------------------------------------\n",
      "epoch: 186 loss avg:0.26351043581962585 accum:16.07413673400879\n",
      "----------------------------------------\n",
      "epoch: 187 loss avg:0.25506719946861267 accum:15.559099197387695\n",
      "----------------------------------------\n",
      "epoch: 188 loss avg:0.2549744248390198 accum:15.55344009399414\n",
      "----------------------------------------\n",
      "epoch: 189 loss avg:0.2520640790462494 accum:15.375908851623535\n",
      "----------------------------------------\n",
      "epoch: 190 loss avg:0.2526478171348572 accum:15.411518096923828\n",
      "----------------------------------------\n",
      "epoch: 191 loss avg:0.24541805684566498 accum:14.970501899719238\n",
      "----------------------------------------\n",
      "epoch: 192 loss avg:0.2547781765460968 accum:15.54146957397461\n",
      "----------------------------------------\n",
      "epoch: 193 loss avg:0.24837331473827362 accum:15.150773048400879\n",
      "----------------------------------------\n",
      "epoch: 194 loss avg:0.2592700123786926 accum:15.815471649169922\n",
      "----------------------------------------\n",
      "epoch: 195 loss avg:0.2520209848880768 accum:15.373281478881836\n",
      "----------------------------------------\n",
      "epoch: 196 loss avg:0.2529214322566986 accum:15.428208351135254\n",
      "----------------------------------------\n",
      "epoch: 197 loss avg:0.2554258406162262 accum:15.580977439880371\n",
      "----------------------------------------\n",
      "epoch: 198 loss avg:0.24938274919986725 accum:15.212348937988281\n",
      "----------------------------------------\n",
      "epoch: 199 loss avg:0.2498595267534256 accum:15.241432189941406\n",
      "----------------------------------------\n",
      "epoch: 200 loss avg:0.2511743903160095 accum:15.321638107299805\n",
      "----------------------------------------\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"extra_0\",'rb') as f:\n",
    "    datasets=pickle.load(f)\n",
    "\n",
    "for name in datasets.keys():\n",
    "    \n",
    "    dataset=datasets[name]\n",
    "    dataset['target']=np.array(dataset['target']).reshape(-1)\n",
    "    print(name,dataset['data'].shape)\n",
    "    print('-'*50)\n",
    "    \n",
    "    data,target = dataset['data'],dataset['target']\n",
    "    net = None\n",
    "    if name in ['log', 'square_root', 'square', 'frequency', 'round', 'tanh', 'sigmoid', 'isotonic_regression', 'zscore','normalize']:\n",
    "        net =MLP(400)\n",
    "    elif name in ['sum', 'subtract', 'multiply', 'divide']:\n",
    "        net =MLP(800)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9,weight_decay=0.0001)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0005)\n",
    "\n",
    "    if refine_flag is True:\n",
    "        load_model(\"{}.pkl\".format(name),net,device,mode=\"train\",optimizer=optimizer,lr=0.0001)\n",
    "    else:\n",
    "        net.to(device)\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(data).float(),torch.tensor(target).long())\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "\n",
    "    for epoch in range(201):  # loop over the dataset multiple times\n",
    "        loss_accum = 0.0\n",
    "        for i, (batch_x,batch_y) in enumerate(loader):\n",
    "            batch_x,batch_y = batch_x.to(device),batch_y.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred_y = net(batch_x)\n",
    "            #print(torch.max(pred_y,1))\n",
    "            loss = criterion(pred_y,batch_y)\n",
    "            loss_accum = loss_accum + loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('epoch: {} loss avg:{} accum:{}'.format(epoch, loss_accum / len(loader), loss_accum))\n",
    "        print('-'*40)\n",
    "        if epoch%10==0:\n",
    "            save_model(\"{}.pkl\".format(name),net,optimizer)\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "'''\n",
    "in: 一个ndarray的2列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 2列每个元素对应相加\n",
    "'''\n",
    "\n",
    "\n",
    "def f_sum(column_1, column_2):\n",
    "    return column_1 + column_2\n",
    "\n",
    "\n",
    "'''\n",
    "in: 一个ndarray的2列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 2列每个元素对应相减\n",
    "'''\n",
    "\n",
    "\n",
    "def f_subtract(column_1, column_2):\n",
    "    return column_1 - column_2\n",
    "\n",
    "\n",
    "'''\n",
    "in: 一个ndarray的2列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 2列每个元素对应相减\n",
    "'''\n",
    "\n",
    "\n",
    "def f_multiply(column_1, column_2):\n",
    "    return column_1 * column_2\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray的2列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 2列每个元素对应相减\n",
    "'''\n",
    "\n",
    "\n",
    "def f_divide(column_1, column_2):\n",
    "    condition = None\n",
    "    if torch.is_tensor(column_2):\n",
    "        condition = torch.all(torch.ne(column_2, 0))\n",
    "    else:\n",
    "        condition = np.all(column_2 != 0)\n",
    "    if condition:\n",
    "        return column_1 / column_2\n",
    "    return None\n",
    "\n",
    "\n",
    "class Binaries:\n",
    "    #name = ['sum', 'subtract', 'multiply', 'divide']\n",
    "    #func = [f_sum, f_subtract, f_multiply, f_divide]\n",
    "    name = ['multiply', 'divide']\n",
    "    func = [f_multiply, f_divide]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def f_log(column):\n",
    "    if torch.is_tensor(column):\n",
    "        return torch.log2(column) if torch.all(torch.gt(column, 0)) else None\n",
    "    else:\n",
    "        return np.log2(column) if np.all(column > 0) else None\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 2列每个元素绝对值对应求平方根\n",
    "'''\n",
    "\n",
    "\n",
    "def f_square_root(column):\n",
    "    return torch.sqrt(torch.abs(column)) if torch.is_tensor(column) else np.sqrt(np.abs(column))\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 2列每个元素对应求平方根,负值对绝对值求平方根加符号,例如square_root(-9)=-3\n",
    "'''\n",
    "\n",
    "\n",
    "def f_square(column):\n",
    "    return torch.sqrt(torch.abs(column)) * torch.sign(column) if torch.is_tensor(column) else np.sqrt(\n",
    "        np.abs(column)) * np.sign(column)\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description: 对应元素替换成该元素在这一列出现的频次,例:[7,7,2,3,3,4] -> [2,2,1,2,2,1]\n",
    "'''\n",
    "\n",
    "\n",
    "def f_frequency(column):\n",
    "    freq = pd.value_counts(np.array(column))\n",
    "    freq_result = list(map(lambda x: freq[x], np.array(column)))\n",
    "    return torch.tensor(freq_result).float() if torch.is_tensor(column) else np.array(freq_result)\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description:每个值对应四舍五入\n",
    "'''\n",
    "\n",
    "\n",
    "def f_round(column):\n",
    "    return torch.round(column) if torch.is_tensor(column) else np.round(column).astype('int')\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1)\n",
    "out: 一个m×1 的 1 列\n",
    "description:每个值对应双曲正切\n",
    "'''\n",
    "\n",
    "\n",
    "def f_tanh(column):\n",
    "    return torch.tanh(column) if torch.is_tensor(column) else np.tanh(column)\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1)\n",
    "out: 一个m×1 的 1 列er\n",
    "description:每个值对应sigmoid,自己查一下sigmoid函数\n",
    "'''\n",
    "\n",
    "\n",
    "def f_sigmoid(column):\n",
    "    return torch.sigmoid(column) if torch.is_tensor(column) else (1 / (1 + np.exp(-column)))\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1),\n",
    "out: 一个m×1 的 1 列\n",
    "description:对该列值的分布进行,自己查一下保序回归\n",
    "'''\n",
    "\n",
    "\n",
    "def f_isotonic_regression(column):\n",
    "    inds = range(column.shape[0])\n",
    "    if torch.is_tensor(column):\n",
    "        return torch.tensor(IsotonicRegression().fit_transform(inds, column)).float()\n",
    "    else:\n",
    "        return IsotonicRegression().fit_transform(inds,column)\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1),\n",
    "out: 一个m×1 的 1 列\n",
    "description:对该列值的分布进行z分数,查一下z分数\n",
    "'''\n",
    "\n",
    "\n",
    "def f_zscore(column):\n",
    "    if torch.is_tensor(column):\n",
    "        mv, stv = torch.mean(column), torch.std(column)\n",
    "        condition = torch.all(torch.ne(stv, 0))\n",
    "    else:\n",
    "        mv, stv = np.mean(column), np.std(column)\n",
    "        condition = np.all(stv != 0)\n",
    "    if condition:\n",
    "        return (column - mv) / stv\n",
    "    return None\n",
    "\n",
    "\n",
    "'''\n",
    "in:  一个ndarray 的1列(m×1),\n",
    "out: 一个m×1 的 1 列\n",
    "description:对该列值的分布进行-1到1正则化,查一下normalization\n",
    "'''\n",
    "\n",
    "\n",
    "def f_normalize(column):\n",
    "    if torch.is_tensor(column):\n",
    "        maxv, minv = torch.max(column), torch.min(column)\n",
    "        condition = torch.equal(maxv, minv)\n",
    "    else:\n",
    "        maxv, minv = np.max(column), np.min(column)\n",
    "        condition = maxv == minv\n",
    "    if condition:\n",
    "        return None\n",
    "    return -1 + 2 / (maxv - minv) * (column - minv)\n",
    "\n",
    "\n",
    "class Unaries:\n",
    "    name = ['log', 'square_root', 'square', 'frequency', 'round', 'tanh', 'sigmoid', 'isotonic_regression', 'zscore']\n",
    "    func = [f_log, f_square_root, f_square, f_frequency, f_round, f_tanh, f_sigmoid, f_isotonic_regression, f_zscore]\n",
    "\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "model_groups = dict()\n",
    "for name in Unaries.name:\n",
    "    model_groups[name] = MLP(400)\n",
    "    load_model(\"{}.pkl\".format(name),model_groups[name],device,mode=\"eval\")\n",
    "for name in Binaries.name:\n",
    "    model_groups[name] = MLP(800)\n",
    "    load_model(\"{}.pkl\".format(name),model_groups[name],device,mode=\"eval\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "threshold = 0.88\n",
    "n_attempt = 1000\n",
    "perm =True\n",
    "\n",
    "def is_continuous(column):\n",
    "    if len(np.unique(column))>3:\n",
    "        return True\n",
    "\n",
    "def probability(out):\n",
    "    out= torch.nn.functional.softmax(out, dim=0)\n",
    "    result = torch.max(out, 0)\n",
    "    #print(result)\n",
    "    return result[0] if result[1] == 1 else 1-result[0]\n",
    "    \n",
    "\n",
    "def get_tensor_sketch(n_bins, feature, labels):\n",
    "    supr, infr = torch.max(feature), torch.min(feature)\n",
    "    idx0, idx1 = torch.where(labels == 0), torch.where(labels == 1)\n",
    "\n",
    "    sketch0 = torch.histc(feature[idx0], bins=n_bins, min=float(infr), max=float(supr))\n",
    "    sketch1 = torch.histc(feature[idx1], bins=n_bins, min=float(infr), max=float(supr))\n",
    "\n",
    "    sketch0 = -10 + 20 * (sketch0 - torch.min(sketch0)) / (torch.max(sketch0) - torch.min(sketch0))\n",
    "    sketch1 = -10 + 20 * (sketch1 - torch.min(sketch1)) / (torch.max(sketch1) - torch.min(sketch1))\n",
    "    quantile_sketch = torch.cat((sketch0, sketch1), 0)\n",
    "    return quantile_sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef=[93, 18, 68, 35, 4, 55, 69, 3, 31, 23, 5, 70, 65, 41, 54, 100, 45, 28, 101, 24, 83, 84, 44, 17, 53, 103, 64, 22, 33, 94, 39, 36, 63, 10, 77, 86, 82, 30, 75, 25, 99, 34, 89, 2, 11, 16, 85, 13, 21, 72, 95, 7, 90, 29, 51, 20, 40, 6, 87, 43, 96, 42, 81, 32, 37, 9, 98, 61, 14, 46, 8, 0, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class InsuranceDataset:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.target = None\n",
    "        self.features = None\n",
    "        self.cont_index = list()\n",
    "    \n",
    "    def read_insurance_csv(self,path,ef):\n",
    "        dataset = pd.read_csv(path)\n",
    "        for col in dataset.columns.values:\n",
    "            mask = dataset[col] != np.inf\n",
    "            dataset.loc[~mask, col] = dataset.loc[mask, col].max()\n",
    "            \n",
    "        if ef is None:\n",
    "            self.data = torch.from_numpy(dataset.values[:,:-1].astype(\"float\")).float()\n",
    "            self.target = torch.from_numpy(dataset.values[:,-1].astype(\"int\")).long()\n",
    "            self.features = list(dataset.columns.values[:-1])\n",
    "        else:\n",
    "            self.data = torch.from_numpy(dataset.values[:,ef].astype(\"float\")).float()\n",
    "            self.target = torch.from_numpy(dataset.values[:,-1].astype(\"int\")).long()\n",
    "            self.features = list(dataset.columns.values[ef])\n",
    "            \n",
    "        \n",
    "        # 获得连续特征的下标\n",
    "        for col in range(self.data.shape[1]):\n",
    "            if is_continuous(self.data[:,col]):\n",
    "                self.cont_index.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "original_class = list()\n",
    "predicted_class = list()\n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    original_class.extend(y_true)\n",
    "    predicted_class.extend(y_pred)\n",
    "    #return classification_report(y_true, y_pred)# print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%                \n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_candidates: 600\n",
      "unary_candidates: 25\n",
      "later feature num: torch.Size([1510, 73])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.61      0.55       300\n",
      "           1       0.90      0.85      0.88      1210\n",
      "\n",
      "    accuracy                           0.80      1510\n",
      "   macro avg       0.70      0.73      0.71      1510\n",
      "weighted avg       0.82      0.80      0.81      1510\n",
      "\n",
      "benchscore: 0.8046357615894039\n",
      "beizhixing_jine multiply zongzichan_baochoulv\n",
      "tensor(0.8804, device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "------------------------------\n",
      "shixin_beizhixing_xinxishuliang multiply zongzichan_baochoulv\n",
      "tensor(0.8806, device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "------------------------------\n",
      "yingshouzhangkuan/yingyeshouru multiply zhuce_ziben\n",
      "tensor(0.9062, device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "------------------------------\n",
      "zichanzongji_tongbi multiply zongzichan_baochoulv\n",
      "tensor(0.8802, device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "------------------------------\n",
      "zichan_fuzhailv multiply zhuce_ziben\n",
      "tensor(0.8978, device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "------------------------------\n",
      "maolirunlv multiply zongzichan_baochoulv\n",
      "tensor(0.8801, device='cuda:0', grad_fn=<MaxBackward0>)\n",
      "------------------------------\n",
      "later feature num: torch.Size([1510, 79])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.62      0.56       300\n",
      "           1       0.90      0.85      0.88      1210\n",
      "\n",
      "    accuracy                           0.81      1510\n",
      "   macro avg       0.71      0.74      0.72      1510\n",
      "weighted avg       0.82      0.81      0.81      1510\n",
      "\n",
      "newscore: 0.8059602649006623\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# 读入数据集\n",
    "insurance_dataset =  InsuranceDataset()\n",
    "\n",
    "insurance_dataset.read_insurance_csv(\"fengxian.csv\",ef)\n",
    "# 全排列获得所有特征\n",
    "binary_candidates = list(permutations(insurance_dataset.cont_index,2))\n",
    "unary_candidates = list(permutations(insurance_dataset.cont_index,1))\n",
    "\n",
    "print(\"binary_candidates:\",len(binary_candidates))\n",
    "print(\"unary_candidates:\",len(unary_candidates))\n",
    "\n",
    "#scores = list()\n",
    "score = None\n",
    "for i in range(1):\n",
    "#     classifier=XGBClassifier(learning_rate=0.01,\n",
    "#                       n_estimators=20,           # 树的个数-10棵树建立xgboost\n",
    "#                       max_depth=8,               # 树的深度\n",
    "#                       min_child_weight = 1,      # 叶子节点最小权重\n",
    "#                       gamma=0.,                  # 惩罚项中叶子结点个数前的参数\n",
    "#                       subsample=1,               # 所有样本建立决策树\n",
    "#                       colsample_btree=1,         # 所有特征建立决策树\n",
    "#                       scale_pos_weight=1,        # 解决样本个数不平衡的问题\n",
    "#                       random_state=27,           # 随机数\n",
    "#                       slient = 0\n",
    "#                            )\n",
    "    classifier = GradientBoostingClassifier(random_state=10)\n",
    "    score = cross_val_score(classifier, insurance_dataset.data, insurance_dataset.target, cv=10, scoring=make_scorer(classification_report_with_accuracy_score)).mean()\n",
    "    print('later feature num:', insurance_dataset.data.shape)\n",
    "    print(classification_report(original_class, predicted_class)) \n",
    "    print(\"benchscore:\", score)\n",
    "    #scores.append(score)\n",
    "#print('original feature num:', insurance_dataset.data.shape)\n",
    "#print(\"benchscore:\", score)\n",
    "\n",
    "for bc in binary_candidates:\n",
    "    idx1,idx2= bc\n",
    "    f1,f2 = insurance_dataset.data[:,idx1],insurance_dataset.data[:,idx2]\n",
    "    sk1 = get_tensor_sketch(200,f1,insurance_dataset.target).to(device)\n",
    "    sk2 = get_tensor_sketch(200,f2,insurance_dataset.target).to(device)\n",
    "    qsa = torch.cat((sk1,sk2),0)\n",
    "    for name,func in zip(Binaries.name,Binaries.func):\n",
    "        output = model_groups[name](qsa)\n",
    "        new_feature_name= \"{} {} {}\".format(insurance_dataset.features[idx1], name,insurance_dataset.features[idx2])\n",
    "        prob = probability(output)\n",
    "        if prob > threshold:\n",
    "            new_feature = func(f1,f2)\n",
    "            if new_feature is not None:\n",
    "                print(new_feature_name)\n",
    "                print(prob)\n",
    "                print(\"-\"*30)\n",
    "                insurance_dataset.features.append(new_feature_name)\n",
    "                new_feature = torch.unsqueeze(new_feature, 1)\n",
    "                insurance_dataset.data = torch.cat((insurance_dataset.data, new_feature), 1)\n",
    "                \n",
    "\n",
    "for uc in unary_candidates:\n",
    "    idx = uc[0]\n",
    "    f = insurance_dataset.data[:,idx]\n",
    "    qsa = get_tensor_sketch(200,f,insurance_dataset.target).to(device)\n",
    "    for name,func in zip(Unaries.name,Unaries.func):\n",
    "        output = model_groups[name](qsa)\n",
    "        new_feature_name=\"{} {}\".format(name,insurance_dataset.features[idx])\n",
    "        prob = probability(output)\n",
    "        if prob>threshold:\n",
    "            new_feature = func(f)\n",
    "            if new_feature is not None:\n",
    "                print(new_feature_name)\n",
    "                print(prob)\n",
    "                print(\"-\"*30)\n",
    "                insurance_dataset.features.append(new_feature_name)\n",
    "                new_feature = torch.unsqueeze(new_feature,1)\n",
    "                insurance_dataset.data = torch.cat((insurance_dataset.data,new_feature),1)\n",
    "\n",
    "score = None\n",
    "for i in range(1):\n",
    "#     classifier=XGBClassifier(learning_rate=0.01,\n",
    "#                       n_estimators=20,           # 树的个数-10棵树建立xgboost\n",
    "#                       max_depth=8,               # 树的深度\n",
    "#                       min_child_weight = 1,      # 叶子节点最小权重\n",
    "#                       gamma=0.,                  # 惩罚项中叶子结点个数前的参数\n",
    "#                       subsample=1,               # 所有样本建立决策树\n",
    "#                       colsample_btree=1,         # 所有特征建立决策树\n",
    "#                       scale_pos_weight=1,        # 解决样本个数不平衡的问题\n",
    "#                       random_state=27,           # 随机数\n",
    "#                       slient = 0\n",
    "#                            )\n",
    "    #score = cross_val_score(classifier, np.array(insurance_dataset.data), np.array(insurance_dataset.target), cv=10, scoring='f1')\n",
    "    classifier = GradientBoostingClassifier()\n",
    "    original_class = list()\n",
    "    predicted_class = list()\n",
    "    score = cross_val_score(classifier, insurance_dataset.data, insurance_dataset.target, cv=10, scoring=make_scorer(classification_report_with_accuracy_score)).mean()\n",
    "    #scores.append(score)\n",
    "    print('later feature num:', insurance_dataset.data.shape)\n",
    "    print(classification_report(original_class, predicted_class)) \n",
    "    print(\"newscore:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
